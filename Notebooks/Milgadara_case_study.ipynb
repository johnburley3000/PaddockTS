{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9802dd1b-7e2f-4b92-8015-bfcd02ac8e8e",
   "metadata": {},
   "source": [
    "# PaddockTS 'digital case study'@Milgadara\n",
    "\n",
    "## Purpose\n",
    "\n",
    "## Input data\n",
    "- paddock polygons, manually drawn (analysis with autogenerated polygons is done in 03_paddock-ts.ipynb)\n",
    "- paddock-level management annotations and yield data\n",
    "- sentinel time series (ds2.pkl)\n",
    "\n",
    "## Steps\n",
    "1. Merge paddock geometries with annotation info.\n",
    "2. Get paddock annotations into a usable format.\n",
    "3. Load ds, get new indicies, estimate veg cover fractions, resample weekly and interpolate missing. \n",
    "4. save mp4 of RGB and veg fration\n",
    "5. Generate the paddock-variable-week dataset (xarray object) and save this. (this takes a lot of effort to make)\n",
    "\n",
    "## Outputs\n",
    "- Annual heatmap time series (single variable) showing paddock annotations.\n",
    "- Annual paddock clustering (multivariate) with paddock annotations.\n",
    "- Multi-paddock video time series with paddock trackers.\n",
    "- 2017-2024 calendar plot for each paddock with crop type printed in the margin for each year. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d55e97-10ab-43e5-aed8-436a747427f0",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2979edef-ab46-4bec-814d-b7471fe719fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 07:55:09.355007: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-15 07:55:09.355428: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-15 07:55:09.439946: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import rasterio #\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "import rioxarray\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2 \n",
    "from matplotlib.gridspec import GridSpec\n",
    "from dea_tools.plotting import display_map, rgb, xr_animation\n",
    "import skimage\n",
    "\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.preprocessing import TimeSeriesResampler\n",
    "from tslearn.clustering import KShape, KernelKMeans, silhouette_score\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Video\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import ffmpeg # REQUIRES a module load ffmpeg/4.3.1 (in jupyterlab, must do when setting up sesh)\n",
    "\n",
    "# for veg cover fraction part:\n",
    "import tensorflow as tf\n",
    "from fractionalcover3 import unmix_fractional_cover\n",
    "from fractionalcover3 import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc05ddd4-c09d-4e58-8869-9b9bf7ee9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/106/jb5097/Projects/PaddockTS')\n",
    "\n",
    "stub = 'MILG_b033_2017-24'\n",
    "outdir = \"/g/data/xe2/John/Data/PadSeg/\" # best if output is stored in gdata\n",
    "\n",
    "# Temporary directory for animations, frames and plots\n",
    "# clear the tmp directory and create anew\n",
    "tmp_dir = '/scratch/xe2/jb5097/tmp2/'+stub+'/'\n",
    "shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "os.makedirs(tmp_dir)\n",
    "\n",
    "paddocks_manual = \"/g/data/xe2/John/Data/PadSeg/milg_manualpaddocks2.gpkg\" # hand-drawn paddock polygons with name column that MAY match with annotation data (not all rows will have annotations)\n",
    "paddock_annotations = \"/g/data/xe2/John/Data/PadSeg/MILG_paddocks_tmp.csv\" # the latest version of paddock management annotation data (assumes format stays the same!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be20ec-d1a5-43b9-9169-23d17f5219fd",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d4fa9f-a62e-464b-9436-10ed4fde4ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncreate paddocks-variable-time xarray given polygons and xarray\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "create paddocks-variable-time xarray given polygons and xarray\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29bed886-e025-4255-b95d-8439f9227073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_add_fractional_cover(ds, band_names, i, correction=True):\n",
    "    \"\"\"\n",
    "    Calculate the fractional cover using specified bands from an xarray Dataset \n",
    "    and add the results as new bands to the original Dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The input xarray Dataset containing the satellite data.\n",
    "    band_names (list): A list of 6 band names to use for the calculation.\n",
    "    i (int): The integer specifying which pretrained model to use.\n",
    "    correction (bool): Whether to apply correction factors to the input bands.\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: The updated xarray Dataset with the new fractional cover bands.\n",
    "    \"\"\"\n",
    "    # Check if the number of band names is exactly 6\n",
    "    if len(band_names) != 6:\n",
    "        raise ValueError(\"Exactly 6 band names must be provided\")\n",
    "    \n",
    "    # Extract the specified bands and stack them into a numpy array with shape (time, bands, x, y)\n",
    "    inref = np.stack([ds[band].values for band in band_names], axis=1)\n",
    "    print('Shape of input (should be time, bands, x, y):', inref.shape)  # This should now be (time, bands, x, y)\n",
    "\n",
    "    if correction:\n",
    "        print('Using correction factors that attempt to fudge S2 data to better match Landsat.. be careful?')\n",
    "        # Array for correction factors \n",
    "        # This is taken from here: https://github.com/petescarth/fractionalcover/blob/main/notebooks/ApplyModel.ipynb\n",
    "        # and described in a paper by Neil Flood for taking Landsat to Sentinel 2 reflectance (and visa versa).\n",
    "        # NOT SURE THIS IS BEING IMPLEMENTED PROPERLY> THINK ABOUT ORDER OF OPERATION CT LINKED NOTEBOOK\n",
    "        correction_factors = np.array([0.9551, 1.0582, 0.9871, 1.0187, 0.9528, 0.9688]) + \\\n",
    "                             np.array([-0.0022, 0.0031, 0.0064, 0.012, 0.0079, -0.0042])\n",
    "    \n",
    "        # Apply correction factors using broadcasting\n",
    "        inref = inref * correction_factors[:, np.newaxis, np.newaxis]\n",
    "    else:\n",
    "        print('Not applying correction factors')\n",
    "        inref = inref * 0.0001  # if not applying the correction factors\n",
    "\n",
    "    # Initialize an array to store the fractional cover results\n",
    "    fractions = np.empty((inref.shape[0], 3, inref.shape[2], inref.shape[3]))\n",
    "\n",
    "    # Loop over each time slice and apply the unmix_fractional_cover function\n",
    "    for t in range(inref.shape[0]):\n",
    "        fractions[t] = unmix_fractional_cover(inref[t], fc_model=data.get_model(n=i))\n",
    "\n",
    "    # Create DataArray for each vegetation fraction\n",
    "    bg = xr.DataArray(fractions[:, 0, :, :], coords=[ds.coords['time'], ds.coords['y'], ds.coords['x']], dims=['time', 'y', 'x'])\n",
    "    pv = xr.DataArray(fractions[:, 1, :, :], coords=[ds.coords['time'], ds.coords['y'], ds.coords['x']], dims=['time', 'y', 'x'])\n",
    "    npv = xr.DataArray(fractions[:, 2, :, :], coords=[ds.coords['time'], ds.coords['y'], ds.coords['x']], dims=['time', 'y', 'x'])\n",
    "    \n",
    "    # Assign new DataArrays to the original Dataset\n",
    "    ds_updated = ds.assign(bg=bg, pv=pv, npv=npv)\n",
    "    \n",
    "    return ds_updated\n",
    "\n",
    "# # Example usage:\n",
    "# band_names = ['nbart_blue', 'nbart_green', 'nbart_red', 'nbart_nir_2', 'nbart_swir_2', 'nbart_swir_3']\n",
    "# i = 1  # or whichever model index you want to use\n",
    "# ds_updated = calculate_and_add_fractional_cover(ds, band_names, i)\n",
    "# print(ds_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd614279-edef-4830-9507-0c67fe1c3dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to add selected spectral indices to an xarray\n",
    "def calculate_indices(ds, indices):\n",
    "    \"\"\"\n",
    "    Calculate multiple indices and add them to the dataset, retaining all data variables.\n",
    "    \n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): The input xarray dataset with dimensions (paddock, variable, time)\n",
    "    indices (dict): A dictionary where keys are the names of the indices to be added, \n",
    "                    and values are functions that calculate the index.\n",
    "    \n",
    "    Returns:\n",
    "    xarray.Dataset: The dataset with the additional indices and all original data variables.\n",
    "    \"\"\"\n",
    "    new_bands = []\n",
    "\n",
    "    for index_name, index_func in indices.items():\n",
    "        # Calculate the index\n",
    "        index_data = index_func(ds)\n",
    "        \n",
    "        # Expand dimensions of the new index to match the original dataset structure\n",
    "        index_expanded = index_data.expand_dims(variable=[index_name], axis=1)\n",
    "        \n",
    "        # Append the new index to the list\n",
    "        new_bands.append(index_expanded)\n",
    "        print(index_name, 'has shape: ', index_data.shape)\n",
    "    \n",
    "    # Concatenate all new indices along the variable dimension\n",
    "    new_bands_concat = xr.concat([ds.pvt] + new_bands, dim='variable').to_dataset(name='pvt')\n",
    "\n",
    "    # Add back all original data variables to the new dataset\n",
    "    for var in ds.data_vars:\n",
    "        if var != 'pvt':  # Avoid overwriting the 'pvt' variable\n",
    "            new_bands_concat[var] = ds[var]\n",
    "    \n",
    "    return new_bands_concat\n",
    "\n",
    "def calculate_ndvi(ds):\n",
    "    '''NDVI, but why isnt it the same as that downloaded from DEA?\n",
    "    '''\n",
    "    red = ds.sel(variable='nbart_red').pvt\n",
    "    nir = ds.sel(variable='nbart_nir_1').pvt\n",
    "    ndvi = (nir - red) / (red + nir)\n",
    "    return ndvi\n",
    "\n",
    "def calculate_cfi(ds):\n",
    "    '''Calculate CFI (Canola Flower Index)\n",
    "    Tian et al 2022 Remote Sensing https://www.mdpi.com/2072-4292/14/5/1113#sec2dot4-remotesensing-14-01113'''\n",
    "    ndvi = ds.sel(variable='NDVI').pvt\n",
    "    red = ds.sel(variable='nbart_red').pvt\n",
    "    green = ds.sel(variable='nbart_green').pvt\n",
    "    blue = ds.sel(variable='nbart_blue').pvt\n",
    "    \n",
    "    sum_red_green = red + green\n",
    "    diff_green_blue = green - blue\n",
    "    \n",
    "    cfi = ndvi * (sum_red_green + diff_green_blue)\n",
    "    return cfi\n",
    "\n",
    "def calculate_nirv(ds):\n",
    "    '''Near Infrared Reflectance of Vegetation\n",
    "    '''\n",
    "    ndvi = ds.sel(variable='NDVI').pvt\n",
    "    nir = ds.sel(variable='nbart_nir_1').pvt\n",
    "    nirv = ndvi * nir\n",
    "    return nirv\n",
    "\n",
    "def calculate_dnirv(ds):\n",
    "    '''Calculate difference in NIRv compared to previous time step\n",
    "    Caution: this seems to remove one time step\n",
    "    This is currently not working well'''\n",
    "    nirv = calculate_nirv(ds)\n",
    "    dnirv = nirv.diff(dim='time', n=1)\n",
    "    #dnirv = xr.concat([xr.DataArray([0], dims='time'), dnirv], dim='time')  # Handle first time step (make dnirv equal to zero)\n",
    "    return dnirv\n",
    "\n",
    "def calculate_ndti(ds):\n",
    "    \"\"\" Normalized Difference Tillage Index (NDTI).\n",
    "    NDTI = (R1610−R2200)/(R1610 + R2200)\n",
    "    Described here and ref within: https://www.mdpi.com/2072-4292/13/18/3718 \n",
    "    \"\"\"\n",
    "    # Extract the SWIR1 and SWIR2 bands\n",
    "    swir1 = ds.sel(variable='nbart_swir_2').pvt\n",
    "    swir2 = ds.sel(variable='nbart_swir_3').pvt\n",
    "    \n",
    "    # Calculate the NDTI\n",
    "    ndti = (swir1 - swir2) / (swir1 + swir2)\n",
    "    \n",
    "    return ndti\n",
    "\n",
    "def calculate_cai(ds):\n",
    "    \"\"\"Cellulose Absorption Index (CAI).\n",
    "    CAI = (0.5∗(R2000 +R2200))−R2100\n",
    "    see https://www.mdpi.com/2072-4292/13/18/3718 \n",
    "    see also for calibration/nuance with Sentinel data: https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11155/2533761/Identification-of-non-photosynthetic-vegetation-areas-in-Sentinel-2-satellite/10.1117/12.2533761.full\n",
    "    \"\"\"\n",
    "    # Extract the SWIR1, SWIR2, and NIR bands\n",
    "    swir1 = ds.sel(variable='nbart_swir_2').pvt\n",
    "    swir2 = ds.sel(variable='nbart_swir_3').pvt\n",
    "    nir = ds.sel(variable='nbart_nir_1').pvt\n",
    "    \n",
    "    # Calculate the CAI\n",
    "    cai = 0.5 * (swir1 + swir2) - nir\n",
    "    \n",
    "    return cai\n",
    "\n",
    "# # Example usage\n",
    "# indices = {\n",
    "#     'CFI': calculate_cfi,\n",
    "#     'NIRv': calculate_nirv\n",
    "# }\n",
    "\n",
    "# updated_ds = calculate_indices(ds_paddocks_weekly, indices)\n",
    "# print(updated_ds.variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a8594-6078-4380-996a-70deb680caec",
   "metadata": {},
   "source": [
    "### Load data\n",
    "(for everything except paddock-year yield analysis. For that, skip to bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f1c2e83-4701-4745-898c-1c4881d68d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. vars: 23\n"
     ]
    }
   ],
   "source": [
    "# Read in the polygons from SAMGeo (these will not neccesarily match user-provided paddocks)\n",
    "pol = gpd.read_file(outdir+stub+'_filt.gpkg')\n",
    "\n",
    "# have to set a paddock id. Preferably do this in earlier step in future... \n",
    "pol['paddock'] = range(1,len(pol)+1)\n",
    "pol['paddock'] = pol.paddock.astype('category')\n",
    "\n",
    "# Read in the array of paddocks by variables (e.g. bands) by time -- the pvt array\n",
    "pvt = np.load(outdir+stub+'_pvt.npy')\n",
    "# pvt = np.load(data_path+stub+'_pvt2.npy')\n",
    "\n",
    "# get the variable names:\n",
    "with open(outdir+stub+'_pvt_vars.pkl', 'rb') as handle:\n",
    "    var_names = pickle.load(handle)\n",
    "print('No. vars:',len(var_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33240ab7-e987-43f4-967f-74fb70583900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input (should be time, bands, x, y): (417, 6, 699, 638)\n",
      "Not applying correction factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Open the satellite data stack\n",
    "#year = 2023\n",
    "year = None\n",
    "\n",
    "with open(outdir+stub+'_ds2.pkl', 'rb') as handle:\n",
    "    ds = pickle.load(handle)\n",
    "    # Filter the data if year is not null\n",
    "    if year is not None:\n",
    "        ds = ds.sel(time=ds['time'].dt.year == year)\n",
    "\n",
    "## Add veg fractions to ds\n",
    "band_names = ['nbart_blue', 'nbart_green', 'nbart_red', 'nbart_nir_1', 'nbart_swir_2', 'nbart_swir_3']\n",
    "i = 3  # or whichever model index you want to use\n",
    "ds = calculate_and_add_fractional_cover(ds, band_names, i, correction=False)\n",
    "\n",
    "# Resample data weekly\n",
    "ds_weekly = ds.resample(time=\"1W\").interpolate(\"linear\")\n",
    "\n",
    "print('No. weeks in time series:', len(ds_weekly.time.values))\n",
    "ds_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd741c56-4940-4357-b697-8889ae8be60a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in manual polygons and paddock annotation data. Merge and keep as a geopandas df:\n",
    "\n",
    "# paddock annotatioun data:\n",
    "pad_an = pd.read_csv(paddock_annotations)\n",
    "\n",
    "# Load the manual drawn polygons GeoDataFrame\n",
    "pad_man = gpd.read_file(paddocks_manual)\n",
    "\n",
    "# Remove rows that have no geometry\n",
    "pad_man = pad_man[pad_man.geometry.notnull()]\n",
    "print(len(pad_man))\n",
    "\n",
    "# Identify rows with invalid geometries\n",
    "invalid_geometries = pad_man[~pad_man.is_valid]\n",
    "print(\"Invalid geometries:\")\n",
    "print(invalid_geometries)\n",
    "\n",
    "# Remove rows with invalid geometries\n",
    "pad_man = pad_man[pad_man.is_valid]\n",
    "print(\"------\")\n",
    "print(len(pad_man))\n",
    "\n",
    "# Add a new column with unique numbers\n",
    "pad_man['paddock'] = range(1, len(pad_man) + 1)\n",
    "\n",
    "# merge manual drawn polygons with annotations\n",
    "pad_manan = pd.merge(pad_man, pad_an, left_on='name', right_on='Name', how='left').drop(columns=['Name'])\n",
    "print(pad_manan.crs)\n",
    "print(len(pad_manan), len(pad_man), len(pad_an))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba73a7-7157-40c6-9df3-62566e654eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add colors to plot the polygons with:\n",
    "# Create the 'color' column and set it to None for all rows\n",
    "pad_man['color'] = 'None'\n",
    "\n",
    "# Define the conditions and corresponding values for 'edge_color'\n",
    "conditions = [\n",
    "    (pad_man['type'] == 'forest'),\n",
    "    (pad_man['type'] == 'tree_row'),\n",
    "    (pad_man['type'] == 'named'),\n",
    "    (pad_man['type'] == 'unnamed'),\n",
    "    # Add more conditions here if needed\n",
    "]\n",
    "\n",
    "values = ['green', 'yellow', 'red', 'blue']\n",
    "# Add corresponding values for additional conditions here if needed\n",
    "\n",
    "# Create the 'edge_color' column based on the conditions\n",
    "pad_man['edge_color'] = np.select(conditions, values, default='other')\n",
    "print(pad_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585ed11-99cf-472d-b24b-7bf33c0d6710",
   "metadata": {},
   "source": [
    "### Visual checks of data before analysis\n",
    "- map the paddocks\n",
    "- animate the RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c30df-d2a9-4c37-b078-41a508c188e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour the paddocks according to whether named, un-named, forest block, tree row. \n",
    "\n",
    "# num_frames = 10\n",
    "# xr_animation(ds_weekly, \n",
    "#              bands = ['nbart_red', 'nbart_green', 'nbart_blue'], \n",
    "#              output_path = tmp_dir+'quick_animation.mp4', \n",
    "#              show_gdf = pad_man, \n",
    "#              gdf_kwargs={\"edgecolor\": pad_man['edge_color']}, \n",
    "#              #gdf_kwargs={\"edgecolor\": 'red'}, \n",
    "#              limit = num_frames)\n",
    "# plt.close()\n",
    "# Video(tmp_dir+'quick_animation.mp4', embed = True)\n",
    "\n",
    "# This version only shows the labelled paddocks\n",
    "pol = pad_man[pad_man['type'] == 'named']\n",
    "num_frames = len(ds_weekly.time)\n",
    "xr_animation(ds_weekly, \n",
    "             bands = ['nbart_red', 'nbart_green', 'nbart_blue'], \n",
    "             output_path = outdir+stub+'manpad_RGB.mp4', \n",
    "             show_gdf = pol, \n",
    "             #gdf_kwargs={\"edgecolor\": pol['edge_color']}, \n",
    "             gdf_kwargs={\"edgecolor\": 'white'}, \n",
    "             limit = num_frames)\n",
    "plt.close()\n",
    "Video(outdir+stub+'manpad_RGB.mp4', embed = True) # rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066727ba-b28c-447a-997c-04d3975b57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(pad_man.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3151c5-d036-4fe0-bd68-fa14f99d2d45",
   "metadata": {},
   "source": [
    "### Paddocks x variable x time\n",
    "- clustering using heirachical clustering and/or time series clustering\n",
    "- make interactive heat map to hover over labels (single variable)\n",
    "- make interactive pairwise dissimilarity matrix (multi variable)\n",
    "- dimensionality reduction plot (k-means? tSNE?), also interactive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0c1f4-b8dd-4888-9b72-c9f77b3283a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to process each geometry row\n",
    "def process_geometry(datarow, ds):\n",
    "    import rioxarray\n",
    "    \"\"\"\n",
    "    Process each geometry to extract the median band values.\n",
    "    Args:\n",
    "        datarow: A row from the geopandas dataframe containing the geometry.\n",
    "        ds: The xarray dataset with time series satellite imagery.\n",
    "    Returns:\n",
    "        A numpy array of median band values for the geometry over time.\n",
    "    \"\"\"\n",
    "    # Clip the xarray dataset to the polygon\n",
    "    ds_clipped = ds.rio.clip([datarow.geometry])\n",
    "\n",
    "    # Extract the median band value, ignoring zero values\n",
    "    pol_ts = ds_clipped.where(ds_clipped > 0).median(dim=['x', 'y'])\n",
    "    array = pol_ts.to_array().transpose('variable', 'time').values.astype(np.float32)\n",
    "\n",
    "    return array[None, :]\n",
    "\n",
    "# Use parallel processing to extract time series data for each paddock\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_geometry)(datarow, ds_weekly) \n",
    "    for datarow in tqdm(pad_man.itertuples(index=True), total=len(pad_man))\n",
    ")\n",
    "\n",
    "# Combine the results into a single numpy array\n",
    "pvt = np.vstack([res for res in results])\n",
    "\n",
    "print(\"Processing complete\")\n",
    "print(\"pvt shape: \", pvt.shape)\n",
    "\n",
    "## Should turn this all into a clean function that outputs the pvt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88525196-589c-4886-ae8b-c3bd3ff51c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ds_paddocks\n",
    "def create_paddock_xarray(pol, pvt_array, ds):\n",
    "    '''TO DO\n",
    "    remove valid_crown_ids\n",
    "    '''\n",
    "    # Extract time axis from the xarray Dataset\n",
    "    time_axis = ds.time\n",
    "\n",
    "    # get variable names (bands)\n",
    "    var_names = list(ds.data_vars.keys())\n",
    "\n",
    "    # Create a DataArray for the paddock geometries\n",
    "    geometry_da = xr.DataArray(pol.geometry.values, dims=[\"paddock\"], name=\"geometry\")\n",
    "\n",
    "    # Create DataArrays for additional variables\n",
    "    name_da = xr.DataArray(pol.name.values, dims=[\"paddock\"], name=\"name\")\n",
    "    type_da = xr.DataArray(pol['type'].values, dims=[\"paddock\"], name=\"type\")\n",
    "\n",
    "    # Create DataArray for each band over time\n",
    "    pvt_da = xr.DataArray(\n",
    "        pvt_array,\n",
    "        dims=[\"paddock\", \"variable\", \"time\"],\n",
    "        coords={\n",
    "            \"paddock\": pol.paddock.values,\n",
    "            \"variable\": var_names,\n",
    "            \"time\": time_axis\n",
    "        },\n",
    "        name=\"pvt\"\n",
    "    )\n",
    "\n",
    "    # Combine into a single Dataset\n",
    "    ds_paddocks = xr.Dataset({\n",
    "        \"geometry\": geometry_da,\n",
    "        \"name\": name_da,\n",
    "        \"type\": type_da,\n",
    "        \"pvt\": pvt_da\n",
    "    })\n",
    "\n",
    "    return ds_paddocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20718e6f-1a9e-4f20-8416-f9ced42192ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_weekly_paddocks = create_paddock_xarray(pad_man, pvt, ds_weekly)\n",
    "ds_weekly_paddocks = calculate_indices(ds_weekly_paddocks, indices)\n",
    "print(ds_weekly_paddocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b0b8ae-5741-4b55-bb78-6eca09051962",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Save ds_weekly_paddocks, as we eventually want to open these for each year. \n",
    "\n",
    "with open(outdir+stub+'_ds_weekly_paddocks_'+str(year)+'.pkl', 'wb') as handle:\n",
    "    pickle.dump(ds_weekly_paddocks, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf07de26-57e2-4421-ba33-4151e4e9c586",
   "metadata": {},
   "source": [
    "### Clustering/heatmaps\n",
    "- produce interactive plots of single variable paddoc time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45368388-39ae-4b47-8907-124ad9d4e463",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_clustermap1(ds_paddocks, variable_name, outdir, stub):\n",
    "    '''\n",
    "    '''\n",
    "    # Check if the variable_name exists in the dataset\n",
    "    if variable_name not in ds_paddocks.variable.values:\n",
    "        raise ValueError(f\"Variable '{variable_name}' not found in the dataset.\")\n",
    "    \n",
    "    # Extract the time series for the specified variable for all paddocks\n",
    "    pt_variable = ds_paddocks.sel(variable=variable_name).pvt.values\n",
    "    \n",
    "    # Interpolate missing values along the time axis (2nd axis)\n",
    "    pt_variable = np.apply_along_axis(\n",
    "        lambda x: pd.Series(x).interpolate(method='linear', limit_direction='both').to_numpy(), \n",
    "        axis=1, \n",
    "        arr=pt_variable\n",
    "    )\n",
    "\n",
    "    print(pt_variable.shape)\n",
    "    # Count NaN values\n",
    "    nan_count = np.sum(np.isnan(pt_variable))\n",
    "    print(f\"Number of NaN values: {nan_count}\")\n",
    "    print(\"replacing nans with 0, for now, so the clustering can work..\")\n",
    "    pt_variable = np.nan_to_num(pt_variable, nan=0)\n",
    "\n",
    "    # Extract the timestamps and convert to Pandas DatetimeIndex\n",
    "    time_stamps = ds_paddocks.time.values\n",
    "    time_index = pd.to_datetime(time_stamps)\n",
    "\n",
    "    # Adjust start date to include January if necessary\n",
    "    start_date = time_index.min()\n",
    "    if start_date.month != 1:\n",
    "        start_date = pd.Timestamp(year=start_date.year, month=1, day=1)\n",
    "    \n",
    "    # Ensure January is included in monthly_start\n",
    "    monthly_start = pd.date_range(start=start_date, end=time_index.max(), freq='MS')\n",
    "\n",
    "    # Find the closest previous timestamps in the original time_index\n",
    "    monthly_ticks = []\n",
    "    for date in monthly_start:\n",
    "        prior_dates = time_index[time_index <= date]\n",
    "        if not prior_dates.empty:\n",
    "            monthly_ticks.append(prior_dates[-1])\n",
    "\n",
    "    monthly_ticks_str = [str(t)[:10] for t in monthly_ticks]\n",
    "\n",
    "    # Extract paddock IDs for row names\n",
    "    row_names = ds_paddocks.name.values\n",
    "\n",
    "    # Extract species types and create a color mapping\n",
    "    species_types = ds_paddocks.type.values\n",
    "    unique_types = np.unique(species_types)\n",
    "    palette = sns.color_palette(\"hsv\", len(unique_types))\n",
    "    type_color_map = {species: palette[i] for i, species in enumerate(unique_types)}\n",
    "    species_colors = np.array([type_color_map[species] for species in species_types])\n",
    "\n",
    "    # Plot heatmap with clustering of the rows\n",
    "    plt.figure(figsize=(6, 12))  # Adjust the figure size as needed\n",
    "    \n",
    "    g = sns.clustermap(pt_variable, method='average', metric='euclidean', \n",
    "                       row_cluster=True, col_cluster=False, cmap='viridis',\n",
    "                       row_colors=species_colors)\n",
    "\n",
    "    # Get the order of the rows after clustering\n",
    "    row_order = g.dendrogram_row.reordered_ind\n",
    "\n",
    "    # Reorder the row names and colors according to the clustering\n",
    "    ordered_row_names = [row_names[i] for i in row_order]\n",
    "    ordered_species_colors = [species_colors[i] for i in row_order]\n",
    "\n",
    "    # Customize the plot\n",
    "    g.ax_heatmap.set_xlabel('Time')\n",
    "    #g.ax_heatmap.set_ylabel('Paddock')\n",
    "\n",
    "    # Set the x-tick labels to the closest previous valid timestamp of each month\n",
    "    tick_positions = [time_index.get_loc(t) for t in monthly_ticks]\n",
    "    g.ax_heatmap.set_xticks(tick_positions)\n",
    "    g.ax_heatmap.set_xticklabels(monthly_ticks_str, rotation=45, ha='right')\n",
    "\n",
    "    # Set the y-tick labels to the ordered paddock IDs\n",
    "    g.ax_heatmap.set_yticks(np.arange(len(ordered_row_names)) + 0.5)\n",
    "    g.ax_heatmap.set_yticklabels(ordered_row_names, fontsize=8, rotation=0)\n",
    "\n",
    "    # Customize the color bar and position it to the right\n",
    "    g.cax.set_position([1.1, 0.2, 0.03, 0.45])  # [left, bottom, width, height]\n",
    "    g.cax.set_title(variable_name, pad=10)  # Title above the color bar\n",
    "\n",
    "    # Create species legend\n",
    "    legend_handles = [mpatches.Patch(color=palette[i], label=species) \n",
    "                      for i, species in enumerate(unique_types)]\n",
    "    \n",
    "    # Add the legend to the plot\n",
    "    plt.legend(handles=legend_handles, title='Paddock type', bbox_to_anchor=(1.05, 1.4), loc='upper left')\n",
    "\n",
    "    # Save the plot to results\n",
    "    plt.savefig(outdir + stub + f\"_pt-{variable_name}.png\", bbox_inches='tight')\n",
    "\n",
    "    # Print the number of missing pixels across all time series for the variable\n",
    "    print(f'Number of missing pixels across all {variable_name} time series:', np.count_nonzero(np.isnan(pt_variable)))\n",
    "\n",
    "def plot_clustermap2(ds_paddocks, variable_name, outdir, stub):\n",
    "    '''This version only shows the named paddocks, and therefore does not color rows by 'type'.\n",
    "    Also, attach crop type to the row label. \n",
    "    Put colour bar above plot to make more room for row labels...\n",
    "    '''\n",
    "    # Check if the variable_name exists in the dataset\n",
    "    if variable_name not in ds_paddocks.variable.values:\n",
    "        raise ValueError(f\"Variable '{variable_name}' not found in the dataset.\")\n",
    "    \n",
    "    # Extract the time series for the specified variable for all paddocks\n",
    "    pt_variable = ds_paddocks.sel(variable=variable_name).pvt.values\n",
    "    \n",
    "    # Interpolate missing values along the time axis (2nd axis)\n",
    "    pt_variable = np.apply_along_axis(\n",
    "        lambda x: pd.Series(x).interpolate(method='linear', limit_direction='both').to_numpy(), \n",
    "        axis=1, \n",
    "        arr=pt_variable\n",
    "    )\n",
    "\n",
    "    print(pt_variable.shape)\n",
    "    # Count NaN values\n",
    "    nan_count = np.sum(np.isnan(pt_variable))\n",
    "    print(f\"Number of NaN values: {nan_count}\")\n",
    "    print(\"replacing nans with 0, for now, so the clustering can work..\")\n",
    "    pt_variable = np.nan_to_num(pt_variable, nan=0)\n",
    "\n",
    "    # Extract the timestamps and convert to Pandas DatetimeIndex\n",
    "    time_stamps = ds_paddocks.time.values\n",
    "    time_index = pd.to_datetime(time_stamps)\n",
    "\n",
    "    # Adjust start date to include January if necessary\n",
    "    start_date = time_index.min()\n",
    "    if start_date.month != 1:\n",
    "        start_date = pd.Timestamp(year=start_date.year, month=1, day=1)\n",
    "    \n",
    "    # Ensure January is included in monthly_start\n",
    "    monthly_start = pd.date_range(start=start_date, end=time_index.max(), freq='MS')\n",
    "\n",
    "    # Find the closest previous timestamps in the original time_index\n",
    "    monthly_ticks = []\n",
    "    for date in monthly_start:\n",
    "        prior_dates = time_index[time_index <= date]\n",
    "        if not prior_dates.empty:\n",
    "            monthly_ticks.append(prior_dates[-1])\n",
    "\n",
    "    monthly_ticks_str = [str(t)[:10] for t in monthly_ticks]\n",
    "\n",
    "    # Extract the crop type information for the specified year\n",
    "    crop_col = f'{year}_Crop'\n",
    "    the_crops = pad_manan[pad_manan['type'] == 'named'][crop_col].fillna('') #crops planted this year for named paddocks only. \n",
    "    the_crops = the_crops.apply(lambda x: x.strip() if isinstance(x, str) else x).replace('', '')\n",
    "    the_crops\n",
    "\n",
    "    # get paddock names\n",
    "    row_names = ds_paddocks.name.values\n",
    "    \n",
    "    # Merge paddock names and crop types for row labels\n",
    "    if len(row_names) != len(the_crops):\n",
    "        raise ValueError(\"The two lists must be of the same length.\")\n",
    "        \n",
    "    names_crops = [f\"{r} / {c}\" for r, c in zip(row_names, the_crops)]\n",
    "    \n",
    "    # Plot heatmap with clustering of the rows\n",
    "    plt.figure(figsize=(6, 12))  # Adjust the figure size as needed\n",
    "\n",
    "    # from matplotlib.colors import LinearSegmentedColormap\n",
    "    # # Define custom colormap\n",
    "    # colors = ['#8B4513', '#FFFFFF', '#008000']  # Brown, White, Green\n",
    "    # n_bins = 100  # Discretizes the interpolation into bins\n",
    "    # cmap_name = 'custom_diverging'\n",
    "    # custom_cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
    "\n",
    "    # g = sns.clustermap(pt_variable, method='average', metric='euclidean', \n",
    "    #                row_cluster=True, col_cluster=False, cmap=custom_cmap)\n",
    "    \n",
    "    g = sns.clustermap(pt_variable, method='average', metric='euclidean', \n",
    "                       row_cluster=True, col_cluster=False, cmap='viridis')\n",
    "\n",
    "    # Get the order of the rows after clustering\n",
    "    row_order = g.dendrogram_row.reordered_ind\n",
    "\n",
    "    # Reorder the row names according to the clustering\n",
    "    ordered_row_names = [names_crops[i] for i in row_order]\n",
    "\n",
    "    # Customize the plot\n",
    "    g.ax_heatmap.set_xlabel('Time')\n",
    "    #g.ax_heatmap.set_ylabel('Paddock')\n",
    "\n",
    "    # Set the x-tick labels to the closest previous valid timestamp of each month\n",
    "    tick_positions = [time_index.get_loc(t) for t in monthly_ticks]\n",
    "    g.ax_heatmap.set_xticks(tick_positions)\n",
    "    g.ax_heatmap.set_xticklabels(monthly_ticks_str, rotation=45, ha='right')\n",
    "\n",
    "    # Set the y-tick labels to the ordered paddock IDs\n",
    "    g.ax_heatmap.set_yticks(np.arange(len(ordered_row_names)) + 0.5)\n",
    "    g.ax_heatmap.set_yticklabels(ordered_row_names, fontsize=8, rotation=0)\n",
    "\n",
    "    # # Customize the color bar and position it above the heatmap horizontally\n",
    "    # g.cax.set_position([0.25, 1.02, 0.5, 0.02])  # [left, bottom, width, height]\n",
    "    # g.cax.xaxis.set_ticks_position('top')\n",
    "    # g.cax.xaxis.set_label_position('top')\n",
    "    g.cax.set_title(variable_name, pad=10)  # Title above the color bar\n",
    "\n",
    "\n",
    "    # Save the plot to results\n",
    "    plt.savefig(outdir + stub + f\"_pt-{variable_name}.png\", bbox_inches='tight')\n",
    "\n",
    "    # Print the number of missing pixels across all time series for the variable\n",
    "    print(f'Number of missing pixels across all {variable_name} time series:', np.count_nonzero(np.isnan(pt_variable)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f34ecb1-28e2-41b7-9897-2008aa43ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "plot_clustermap1(ds_paddocks=ds_weekly_paddocks, variable_name='NDVI', outdir=outdir, stub=stub+\"_\"+str(year)+'_heatmap_dendro_NDVI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1e90a-6e10-4a0c-8b05-46625752e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "ds_weekly_paddocks_named = ds_weekly_paddocks.where(ds_weekly_paddocks['type'] == 'named', drop=True)\n",
    "the_var='CFI3'\n",
    "plot_clustermap2(ds_paddocks=ds_weekly_paddocks_named, variable_name=the_var, outdir=outdir, stub=stub+\"_\"+str(year)+'_heatmap_dendro_'+the_var+'_namedpaddocks')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9aa0e1-10db-41b3-ab7e-be3605ff51a2",
   "metadata": {},
   "source": [
    "### Read in temporal climatic data\n",
    "- pandas time series would be best. Should be easy to subset the same time frame as the ds, and plot in 'dashboard plot'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d9120-ff32-4fb4-9887-3f55f7161b08",
   "metadata": {},
   "source": [
    "### Create 'Dashboard' plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf33637-93cb-4b20-bbd3-941c778e0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from plotly.offline import plot, iplot\n",
    "import plotly.io as pio\n",
    "\n",
    "def plot_time_series(ds_paddocks, bands, pad_manan, year, outdir, stub):\n",
    "    ''' Plot time series curves for multiple specified bands in a multi-panel interactive plot. '''\n",
    "    \n",
    "    # Extract the time values\n",
    "    time_values = pd.to_datetime(ds_paddocks.time.values)\n",
    "    \n",
    "    # Get the paddock values from ds_paddocks\n",
    "    paddock_values = ds_paddocks.paddock.values\n",
    "    \n",
    "    # Subset pad_manan to only include rows with paddocks in ds_paddocks\n",
    "    pad_manan_subset = pad_manan[pad_manan['paddock'].isin(paddock_values)]\n",
    "    \n",
    "    # Extract the crop type information for the specified year\n",
    "    crop_col = f'{year}_Crop'\n",
    "    pad_manan_subset[crop_col] = pad_manan_subset[crop_col].apply(lambda x: 'Unknown' if pd.isna(x) or x.strip() == '' else x)\n",
    "    crop_types = pad_manan_subset.set_index('paddock')[crop_col].to_dict()\n",
    "    \n",
    "    # Identify unique crop types excluding 'Unknown'\n",
    "    unique_crop_types = pad_manan_subset[crop_col].unique()\n",
    "    unique_crop_types = [ct for ct in unique_crop_types if ct != 'Unknown']\n",
    "    \n",
    "    # Create a color map for each unique crop type\n",
    "    colors = px.colors.qualitative.Plotly  # You can choose a different color palette if needed\n",
    "    num_colors = len(colors)\n",
    "    color_map = {crop_type: colors[i % num_colors] for i, crop_type in enumerate(unique_crop_types)}\n",
    "    color_map['Unknown'] = 'lightgrey'  # Assign light grey to 'Unknown'\n",
    "    \n",
    "    # Create subplots, one for each specified band\n",
    "    fig = make_subplots(\n",
    "        rows=len(bands), cols=1, \n",
    "        shared_xaxes=True, \n",
    "        vertical_spacing=0.05,\n",
    "        subplot_titles=[None] * len(bands)\n",
    "    )\n",
    "    \n",
    "    # Iterate over each specified band to create the subplots\n",
    "    for i, band in enumerate(bands):\n",
    "        if band not in ds_paddocks.variable.values:\n",
    "            raise ValueError(f\"Band '{band}' not found in the dataset.\")\n",
    "        \n",
    "        # Extract data for the current band\n",
    "        band_data = ds_paddocks.sel(variable=band).pvt.values\n",
    "        \n",
    "        # Add traces for each paddock\n",
    "        for paddock_index, paddock_name in enumerate(ds_paddocks.name.values):\n",
    "            paddock_id = paddock_values[paddock_index]\n",
    "            crop_type = crop_types.get(paddock_id, 'Unknown')\n",
    "            line_color = color_map.get(crop_type, 'lightgrey')\n",
    "            \n",
    "            # Print for debugging\n",
    "            #print(paddock_name, crop_type, dict(color=line_color))\n",
    "            \n",
    "            hover_text = f'Paddock: {paddock_name}<br>Crop: {crop_type}'\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=time_values,\n",
    "                    y=band_data[paddock_index],\n",
    "                    mode='lines',\n",
    "                    name=f'Paddock {paddock_name}',\n",
    "                    line=dict(color=line_color),\n",
    "                    hoverinfo='text',\n",
    "                    text=hover_text\n",
    "                ),\n",
    "                row=i+1, col=1\n",
    "            )\n",
    "        \n",
    "        # Update y-axis title for each subplot\n",
    "        fig.update_yaxes(title_text=band, row=i+1, col=1)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=300 * len(bands),  # Adjust height according to the number of subplots\n",
    "        title_text=(f\"Year: {year}. Hover over lines to see paddock name and crop planted: {unique_crop_types}\"),\n",
    "        showlegend=False,\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        margin=dict(l=50, r=50, t=50, b=50)\n",
    "    )\n",
    "    \n",
    "    # Update the layout for a more classic look\n",
    "    fig.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=True)\n",
    "    fig.update_yaxes(showline=True, linewidth=1, linecolor='black', mirror=True)\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(showgrid=False),\n",
    "        yaxis=dict(showgrid=False)\n",
    "    )\n",
    "    \n",
    "    # Display the plot in the Jupyter Notebook\n",
    "    iplot(fig)\n",
    "    \n",
    "    # Save the plot to a file and show it\n",
    "    output_file = outdir + stub + \"_time_series.html\"\n",
    "    plot(fig, filename=output_file, auto_open=False)\n",
    "    print(f\"Interactive plot saved as {output_file}\")\n",
    "    print(unique_crop_types)\n",
    "# Example usage:\n",
    "bands = ['NDVI', 'NIRv', 'CFI3', 'NDTI2', 'CAI']  # Specify the bands you want to plot\n",
    "plot_time_series(ds_paddocks=ds_weekly_paddocks_named, bands=bands, pad_manan=pad_manan, year=year, outdir=outdir, stub=stub+\"_\"+str(year))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a7a9a2-91a9-4666-ac02-442033ea3923",
   "metadata": {},
   "source": [
    "### Create calendar plots for each paddock\n",
    "- For each line (year) include text alongside describing management\n",
    "\n",
    "1. subset the paddocks gpd\n",
    "\n",
    "2. For each row:\n",
    "   - clip the ds_weekly\n",
    "   - make the RGB animation\n",
    "   - make and save the calendar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98946d9-0568-4898-bec7-08e554575bd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "named_pads = list(pad_man[pad_man['type'] == 'named'].name)\n",
    "named_pads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978e775-85de-4329-b9c5-5f9dbd2faff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir+stub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f815d-dc5d-444c-944f-e806098fa33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(outdir, stub):\n",
    "    \"\"\"Reads and prepares the satellite data (xarray) for further processing (e.g. calendar plots).\"\"\"\n",
    "    \n",
    "    band_names = ['nbart_blue', 'nbart_green', 'nbart_red', 'nbart_nir_1', 'nbart_swir_2', 'nbart_swir_3']\n",
    "\n",
    "    # Load the xarray dataset from the pickle file first\n",
    "    with open(outdir + stub + '_ds2.pkl', 'rb') as handle:\n",
    "        ds = pickle.load(handle)[band_names]\n",
    "        \n",
    "    ## Add veg fractions to ds\n",
    "    i = 3  # or whichever model index you want to use\n",
    "    ds = calculate_and_add_fractional_cover(ds, band_names, i, correction=False)\n",
    "\n",
    "    # Resample the dataset to weekly intervals and interpolate\n",
    "    ds_weekly_allyears = ds.resample(time=\"1W\").interpolate(\"linear\")\n",
    "\n",
    "    # Determine the earliest time point in ds_weekly\n",
    "    earliest_time = ds_weekly_allyears.time.values[0]\n",
    "\n",
    "    # Create new time steps in 7-day increments before the earliest time, as long as the dates are in 2017\n",
    "    new_time_steps = []\n",
    "    current_time = pd.Timestamp(earliest_time) - pd.Timedelta(days=7)\n",
    "    while current_time.year == 2017:\n",
    "        new_time_steps.append(current_time)\n",
    "        current_time -= pd.Timedelta(days=7)\n",
    "\n",
    "    # Reverse the order of new_time_steps to ensure they are in ascending order\n",
    "    new_time_steps = new_time_steps[::-1]\n",
    "\n",
    "    # Create a new xarray Dataset for these additional time steps with NaN values\n",
    "    nan_data_vars = {var: (('time',) + ds_weekly_allyears[var].dims[1:], np.full((len(new_time_steps),) + ds_weekly_allyears[var].shape[1:], np.nan)) for var in ds_weekly_allyears.data_vars}\n",
    "    new_coords = {coord: ds_weekly_allyears.coords[coord] for coord in ds_weekly_allyears.coords if coord != 'time'}\n",
    "    new_ds = xr.Dataset(\n",
    "        data_vars=nan_data_vars,\n",
    "        coords={**new_coords, 'time': new_time_steps}\n",
    "    )\n",
    "\n",
    "    # Concatenate the new dataset with ds_weekly\n",
    "    ds_weekly_allyears = xr.concat([new_ds, ds_weekly_allyears], dim='time')\n",
    "\n",
    "    return ds_weekly_allyears\n",
    "\n",
    "# Prepare the dataset once\n",
    "ds_weekly_allyears = prepare_dataset(outdir, stub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2721026-a2ae-47c4-9029-9317eb237210",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_weekly_allyears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ccb2c-92b0-457c-9cfc-edafd176d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calendar_plots(ds_weekly_allyears, pad_names, outdir, stub, pad_man):\n",
    "    '''Generates 'calendar plots' (week by year RGB) paddocks specified in pad_names. Also includes option to use paddock ID instead of name.\n",
    "    This can be used to visualize phenology across years for the same paddock.\n",
    "    '''\n",
    "    # List of paddock names to loop through\n",
    "    print(pad_names)\n",
    "    \n",
    "    # Loop through each geometry in pad_man where 'name' is in pad_names\n",
    "    for idx, row in pad_man[pad_man['name'].isin(pad_names)].iterrows():\n",
    "        geometry = row['geometry']\n",
    "        paddock_id = row['paddock']\n",
    "        paddock_name = row['name']\n",
    "        print(paddock_id, paddock_name, geometry)\n",
    "    \n",
    "        # Clip the dataset to the current geometry\n",
    "        clipped_ds = ds_weekly_allyears.rio.clip([mapping(geometry)])\n",
    "        \n",
    "        # # Select only the red, green, and blue bands\n",
    "        # clipped_ds_rgb = clipped_ds[['nbart_red', 'nbart_green', 'nbart_blue']]\n",
    "\n",
    "        # Remove any time steps in 2024\n",
    "        clipped_ds = clipped_ds.sel(time=clipped_ds['time'].dt.year != 2024)\n",
    "\n",
    "        # Replace NaN values with 0\n",
    "        clipped_ds = clipped_ds.fillna(0)\n",
    "    \n",
    "        # Create output filename\n",
    "        out_name_RGB = outdir + stub + '_calendar_' + paddock_name + '_RGB.png'\n",
    "        out_name_vegfrac = outdir + stub + '_calendar_' + paddock_name + '_vegfrac.png'\n",
    "        \n",
    "        # Run xr_animation\n",
    "        rgb(clipped_ds, \n",
    "            bands=['nbart_red', 'nbart_green', 'nbart_blue'], \n",
    "            robust=True, \n",
    "            size=4,\n",
    "            col=\"time\", \n",
    "            col_wrap=52,  # weekly\n",
    "            savefig_path=out_name_RGB)\n",
    "        plt.close()\n",
    "        \n",
    "        rgb(clipped_ds, \n",
    "            bands=['bg', 'pv', 'npv'], \n",
    "            robust=True, \n",
    "            size=4,\n",
    "            col=\"time\", \n",
    "            col_wrap=52,  # weekly\n",
    "            savefig_path=out_name_vegfrac)\n",
    "        plt.close()\n",
    "        print('Finished: ', out_name_vegfrac)\n",
    "\n",
    "# To do: \n",
    "# 1 update plotting function to show text for year and crop alongside rows\n",
    "# 2 would be good to also indicate which thumbnails are interpolated (grey box outline? colour the text above thumnail?)\n",
    "\n",
    "# add a close plot function.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf96f3ee-5fc8-4de2-a3fc-26578bb8caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad_names = ['No 4', 'Pine Hill', 'Washpool', 'Rocky East']\n",
    "\n",
    "pad_names = pad_man.loc[pad_man['type'] == 'named', 'name'].values\n",
    "pad_names = ['Rocky', 'Rocky East',\n",
    "             'Scramble Paddock', 'Washpool East', 'Bottom Range', 'Rocky South',\n",
    "             'Washpool', 'Rubbish Paddock', 'Contour', 'Fingerboard', \"Johnny's\"]\n",
    "\n",
    "calendar_plots(ds_weekly_allyears, pad_names, outdir, stub, pad_man)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c46b863-9cc7-42b4-b636-22337e81b778",
   "metadata": {},
   "source": [
    "### Create an interactive map of yearly crop type.\n",
    "Next version, add other management info and yeild data, when available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3be4a7-9cde-4a34-b0a5-eb3504c75352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "import json\n",
    "from plotly.offline import plot\n",
    "\n",
    "'''\n",
    "The .html files don't load properly. Probably just make these as a normal plot... \n",
    "'''\n",
    "\n",
    "# Ensure the GeoDataFrame has a valid geometry column\n",
    "# (Assuming pad_manan is already a valid GeoDataFrame with a 'geometry' column)\n",
    "\n",
    "# Reproject the GeoDataFrame to EPSG:4326\n",
    "pad_manan = pad_manan.to_crs(epsg=4326)\n",
    "\n",
    "# Convert the GeoDataFrame to GeoJSON format\n",
    "gdf_json = pad_manan.to_json()\n",
    "\n",
    "# Load GeoJSON data\n",
    "geojson_data = json.loads(gdf_json)\n",
    "\n",
    "# Create a list of hover text\n",
    "pad_manan['hover_text'] = pad_manan.apply(lambda row: f'Paddock: {row[\"name\"]}<br>Crop: {row[f\"{year}_Crop\"]}', axis=1)\n",
    "\n",
    "# Create the interactive map with Plotly Express\n",
    "fig = px.choropleth_mapbox(\n",
    "    pad_manan,\n",
    "    geojson=geojson_data,\n",
    "    locations='paddock',\n",
    "    featureidkey=\"properties.paddock\",\n",
    "    color=f'{year}_Crop',  # Use the crop type for coloring\n",
    "    hover_name='hover_text',\n",
    "    hover_data={f'{year}_Crop': False},  # Hide the crop type from hover data\n",
    "    mapbox_style=\"carto-positron\",\n",
    "    center={\"lat\": pad_manan.geometry.centroid.y.mean(), \"lon\": pad_manan.geometry.centroid.x.mean()},\n",
    "    zoom=10,\n",
    "    opacity=0.5\n",
    ")\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(\n",
    "    title_text=f'Crop Types for the Year {year}',\n",
    "    title_x=0.5,\n",
    "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}\n",
    ")\n",
    "\n",
    "# Display the map in the Jupyter Notebook\n",
    "fig.show()\n",
    "\n",
    "# Save the plot to a file and show it\n",
    "output_file = f'{outdir}/{stub}_{year}_crop_map_.html'\n",
    "plot(fig, filename=output_file, auto_open=False)\n",
    "print(f\"Interactive map saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4d759-6473-4e8f-adf3-73655b417c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_man['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975240d7-7365-46ce-9302-411513eb1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1abc787-2864-4bcd-8048-456e00397be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Static map of labelled paddocks using the Fourier Transform tif\n",
    "\n",
    "# Load the Fourier Transform image\n",
    "raster_path = outdir+stub+'.tif'\n",
    "\n",
    "#pol = padman # which type = named\n",
    "pol = pad_man[pad_man['type'] == 'named']\n",
    "\n",
    "# read raster and convert to RGB\n",
    "with rasterio.open(raster_path) as src:\n",
    "    # Read the three bands\n",
    "    band1 = src.read(1)  # Red\n",
    "    band2 = src.read(2)  # Green\n",
    "    band3 = src.read(3)  # Blue\n",
    "    \n",
    "    # Stack the bands into an RGB image\n",
    "    rgb = np.dstack((band1, band2, band3))\n",
    "    \n",
    "    # Normalize to 0-1\n",
    "    rgb = rgb.astype('float32')\n",
    "    rgb /= rgb.max()\n",
    "    \n",
    "    # Ensure CRS matches the raster image (but don't want to change crs, right?) (removed for now)\n",
    "    #pad_man = pad_man.to_crs(src.crs)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Display the RGB image\n",
    "ax.imshow(rgb, extent=(src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top))\n",
    "\n",
    "# Overlay the paddock polygons\n",
    "pol.plot(ax=ax, facecolor='none', edgecolor='red', linewidth=1)\n",
    "\n",
    "# Add paddock labels\n",
    "# for x, y, label in zip(pol.geometry.centroid.x, pol.geometry.centroid.y, pol['name']):\n",
    "#     ax.text(x, y, label, fontsize=8, weight = 'bold', ha='center', va='center', color='yellow')\n",
    "# Add paddock labels (new line for space)\n",
    "for x, y, label in zip(pol.geometry.centroid.x, pol.geometry.centroid.y, pol['name']):\n",
    "    label_with_newlines = label.replace(' ', '\\n')\n",
    "    ax.text(x, y, label_with_newlines, fontsize=8, ha='center', va='center', color='yellow', weight='bold')\n",
    "\n",
    "# Save the figure with the correct size and resolution\n",
    "plt.savefig(outdir+stub+'_map-padman.tif', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05086d04-4366-41a4-b8df-0cad2b8624af",
   "metadata": {},
   "source": [
    "### Yield x NPP relationships\n",
    "Preliminary analysis of reported paddock yeilds as a function of total season NPP. Using NIRv as proxy for NPP. \n",
    "\n",
    "Requires:\n",
    "- paddock-variable-time xarray data for each year to be created and saved. (e.g. MILG_b033_2017-24_ds_weekly_paddocks_2022.pkl)\n",
    "- paddock annotations including crop and yield (paddock-year-yield.csv)\n",
    "- some function for estimating key phenology transition dates from paddock-level satellite data (to be developed more later...)\n",
    "\n",
    "Steps:\n",
    "1. For each paddock-year, estimate total growth season NPP as (something like) the total NIRv during the crop growth season. Also other vars such as growth season length, etc.\n",
    "2. Plot total estimated NPP by reported yield for each paddock-year. Use plotly to label points by year and crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991f8c0-be75-4e06-b590-e7eba21577ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06893e-9dad-4cfe-ac09-ebf0ac51151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_paddock_year_df(df):\n",
    "    # Drop rows where 'Year' is NaN\n",
    "    df_clean = df.dropna(subset=['Year'])\n",
    "    \n",
    "    # Keep the required columns and rename 'Yield_actuals' to 'Yield'\n",
    "    df_clean = df_clean[['Year', 'Paddock', 'Crop', 'ha', 'Yield_actuals']].rename(columns={'Yield_actuals': 'Yield'})\n",
    "    \n",
    "    # Convert 'Year', 'Paddock', and 'Crop' to categorical types\n",
    "    df_clean['Year'] = pd.to_datetime(df_clean['Year'], format='%Y').dt.year\n",
    "    df_clean['Paddock'] = df_clean['Paddock'].astype('category')\n",
    "    df_clean['Crop'] = df_clean['Crop'].astype('category')\n",
    "    \n",
    "    # Convert 'ha' to integer\n",
    "    df_clean['ha'] = df_clean['ha'].astype('int')\n",
    "    \n",
    "    # Convert 'Yield' to numeric, force errors to NaN\n",
    "    df_clean['Yield'] = pd.to_numeric(df_clean['Yield'], errors='coerce')\n",
    "\n",
    "    df_clean['Yield'].replace(\" \", pd.NA, inplace=True)\n",
    "    \n",
    "    # Drop rows where 'Yield' is NaN or 0\n",
    "    #df_clean = df_clean.dropna(subset=['Yield'])\n",
    "    #df_clean = df_clean[df_clean['Yield'] != 0]\n",
    "    \n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110907c0-ad2b-4e6e-a5d5-53ec3f37f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a newer paddock annotation file that has yeild estimates for every named paddock each year\n",
    "# strip back the comments and etc. to leave paddock name, year, crop, yeild, \n",
    "'''TO DO:\n",
    "Replace the pad_an file at the start of this notebook with this paddock annotation file, as it has more info. \n",
    "'''\n",
    "paddock_annotations2 = \"/g/data/xe2/John/Data/PadSeg/paddock-year-yield.csv\" # the latest version of paddock management annotation data (assumes format stays the same!)\n",
    "pad_year = clean_paddock_year_df(pd.read_csv(paddock_annotations2))\n",
    "print(pad_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434241d-65b3-4a64-a302-ef7e8aa5ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pad_year.dropna()\n",
    "print(len(pad_year))\n",
    "print(len(df))\n",
    "\n",
    "# First plot: Yield by Year\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.boxplot(x='Year', y='Yield', data=df)\n",
    "plt.title('Yield by Year')\n",
    "plt.show()\n",
    "\n",
    "# Second plot: Yield by Paddock and Yield by Crop side-by-side with rotated axes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 6))\n",
    "\n",
    "# Yield by Paddock\n",
    "sns.boxplot(ax=axes[0], y='Paddock', x='Yield', data=df)\n",
    "axes[0].set_title('Yield by Paddock')\n",
    "\n",
    "# Yield by Crop\n",
    "sns.boxplot(ax=axes[1], y='Crop', x='Yield', data=df)\n",
    "axes[1].set_title('Yield by Crop')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Why is this still showing crop types for which there no correspoding row in the df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29044967-9f31-4c59-b55c-c8931fc3774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is very messy. It started out estimating NIRv median for each paddock year, then I added the entire time series (but it is staggerd becasue of years), then added the sum of NIRv during May-Dec.\n",
    "# Next time:\n",
    "# Keep the pad_year df of paddock, year, crop yield separate to the time series data. \n",
    "# Make functions that run through the time series and drop new predictor variables into pad_year \n",
    "# The function for generating new predictors should be structured like the function to get spectral indices. Write functions, then list vars to get, then add them. \n",
    "\n",
    "def paddock_year_ts(pad_year,variable_name, year):\n",
    "    # Ensure the Paddock column is a string for matching with xarray 'name'\n",
    "    pad_year['Paddock'] = pad_year['Paddock'].astype(str)\n",
    "    \n",
    "    # Filter pad_year for the specified year\n",
    "    pad_year_filtered = pad_year[pad_year['Year'] == year]\n",
    "\n",
    "    ##### Open ds_weekly for given year:\n",
    "    with open(outdir+stub+'_ds_weekly_paddocks_'+str(year)+'.pkl', 'rb') as handle:\n",
    "        ds_weekly_paddocks = pickle.load(handle)\n",
    "    \n",
    "    # Extract time coordinates from the xarray dataset\n",
    "    time_coords = ds_weekly_paddocks.time.values\n",
    "    \n",
    "    # Create an empty list to store the results\n",
    "    results = []\n",
    "\n",
    "    for paddock in pad_year_filtered['Paddock'].unique():\n",
    "        # Find the index in ds_weekly_paddocks where the name matches the paddock\n",
    "        matching_paddock = ds_weekly_paddocks.sel(paddock=ds_weekly_paddocks.name == paddock)\n",
    "        \n",
    "        if matching_paddock.paddock.size > 0:\n",
    "            # Extract the time series for the specified variable\n",
    "            variable_data = matching_paddock.sel(variable=variable_name).pvt.values.flatten()\n",
    "            \n",
    "            # Ensure the variable data matches the length of the time index\n",
    "            if len(variable_data) == len(time_coords):\n",
    "                # Calculate the median value ignoring nan values\n",
    "                median_value = np.nanmedian(variable_data)\n",
    "                \n",
    "                # Create a dictionary to hold the row data\n",
    "                row_data = {\n",
    "                    'Year': year,\n",
    "                    'Paddock': paddock,\n",
    "                    f'median_{variable_name}': median_value\n",
    "                }\n",
    "                \n",
    "                # Convert the time series data to a pandas Series\n",
    "                time_series = pd.Series(variable_data, index=time_coords)\n",
    "\n",
    "                # Filter the time series to include only the months May-December\n",
    "                may_december = time_series[time_series.index.month >= 5]\n",
    "                may_december = may_december[may_december.index.month <= 12]\n",
    "                \n",
    "                # Calculate the sum of NIRv during May-December\n",
    "                sum_nirv_may_december = may_december.sum()\n",
    "\n",
    "                row_data = {\n",
    "                    'Year': year,\n",
    "                    'Paddock': paddock,\n",
    "                    f'median_growth_season_sumNIRv': sum_nirv_may_december\n",
    "                }\n",
    "                \n",
    "                # Append the time series data to the row data\n",
    "                row_data.update(time_series.to_dict())\n",
    "                \n",
    "                results.append(row_data)\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Filter the final DataFrame to only include rows corresponding to the specified year\n",
    "    final_df = pad_year_filtered.merge(results_df, on=['Year', 'Paddock'], how='left')\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "#paddock_year_ts(pad_year,variable_name, 2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1786994-8be9-49fb-b988-bcd1d4123d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_name = 'NIRv'\n",
    "\n",
    "all_years_df = pd.DataFrame()\n",
    "for year in range(2018, 2024):\n",
    "    #print(year)\n",
    "    df = paddock_year_ts(pad_year, variable_name, year)\n",
    "    all_years_df = pd.concat([all_years_df, df], ignore_index=True)\n",
    "\n",
    "all_years_df['Year'] = all_years_df['Year'].astype(str)\n",
    "\n",
    "\n",
    "all_years_df\n",
    "\n",
    "## NEXT: Once there is a ds_weekly_paddocks saved for every year, run this function through each year and concatenate the output. Make plotly scatterplot and save with group.\n",
    "# This is a strange dataset because the combinations of year-paddock are never consistent across the years.. So it's a weird patchy df with blocks of nans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c358cce4-153a-4ce8-8e89-43be2c2a95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = all_years_df[all_years_df['Yield']>0]\n",
    "df = df.dropna(subset=['Yield', 'median_growth_season_sumNIRv'])\n",
    "\n",
    "# Fit the linear regression model\n",
    "X = sm.add_constant(df['median_growth_season_sumNIRv'])\n",
    "y = df['Yield']\n",
    "model = sm.OLS(y, X).fit()\n",
    "intercept, slope = model.params\n",
    "\n",
    "# Create the scatter plot with Plotly\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x='median_growth_season_sumNIRv',\n",
    "    y='Yield',\n",
    "    color='Year',\n",
    "    hover_data=['Year', 'Paddock', 'Crop'],\n",
    "    labels={'median_growth_season_sumNIRv': 'Median NIRv', 'Yield': 'Yield'},\n",
    "    title='Scatter plot of Median NIRv vs Yield'\n",
    ")\n",
    "\n",
    "# Add the line of best fit\n",
    "x_vals = np.array([df['median_growth_season_sumNIRv'].min(), df['median_growth_season_sumNIRv'].max()])\n",
    "y_vals = intercept + slope * x_vals\n",
    "fig.add_trace(go.Scatter(x=x_vals, y=y_vals, mode='lines', name='Best Fit Line'))\n",
    "\n",
    "# Update the layout to classic styling\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    xaxis=dict(showgrid=False, linecolor='black'),\n",
    "    yaxis=dict(showgrid=False, linecolor='black'),\n",
    "    title=dict(font=dict(size=20, color='black')),\n",
    "    legend=dict(font=dict(size=12)),\n",
    "    margin=dict(l=40, r=40, t=40, b=40),\n",
    "    autosize=True\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6dde1a-57e5-4826-9288-ad5a33a5e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear model:\n",
    "\n",
    "# Define the independent variable (X) and dependent variable (y)\n",
    "X = df['median_growth_season_sumNIRv']\n",
    "y = df['Yield']\n",
    "\n",
    "# Add a constant to the independent variable matrix\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary of the regression model\n",
    "print(model.summary())\n",
    "\n",
    "# Check the p-value for the coefficient of median_NIRv to test significance\n",
    "p_value = model.pvalues['median_growth_season_sumNIRv']\n",
    "print(f\"P-value for median_NIRv coefficient: {p_value}\")\n",
    "\n",
    "# Interpret the results\n",
    "if p_value < 0.05:\n",
    "    print(\"The relationship between median_growth_season_sumNIRv and Yield is significantly positive.\")\n",
    "else:\n",
    "    print(\"The relationship between median_growth_season_sumNIRv and Yield is not significantly positive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d3c3a-6cb5-461e-b4a7-d4238882a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot with Plotly\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x='median_NIRv',\n",
    "    y='Yield',\n",
    "    color='Year',\n",
    "    hover_data=['Year', 'Paddock', 'Crop'],\n",
    "    labels={'median_NIRv': 'Median NIRv', 'Yield': 'Yield'},\n",
    "    title='Scatter plot of Median NIRv vs Yield'\n",
    ")\n",
    "\n",
    "# Update the layout to classic styling\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    xaxis=dict(showgrid=False, linecolor='black'),\n",
    "    yaxis=dict(showgrid=False, linecolor='black'),\n",
    "    title=dict(font=dict(size=20, color='black')),\n",
    "    legend=dict(font=dict(size=12)),\n",
    "    margin=dict(l=40, r=40, t=40, b=40),\n",
    "    autosize=True\n",
    ")\n",
    "\n",
    "# Save the plot as an HTML file\n",
    "fig.write_html(outdir+stub+\"_NIRv-yield_scatter.html\")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbaac1c-e0ad-40f0-b83d-9ed75bbe2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4094d7-c54e-428e-a8eb-945a5f4f4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
