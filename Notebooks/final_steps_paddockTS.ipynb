{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05172498-3738-48e2-bf4d-e46ef52f7572",
   "metadata": {},
   "source": [
    "# Prepping the final steps of PaddockTS\n",
    "## These are the steps:\n",
    "1. Import the data based on a \"stub\" identifier\n",
    "    - time series Sentinel 1/2 after preprocessing and index calculations (*ds2i.pkl, *ds1.pkl) [note: Sentinel-1 is not currently implemented as not used currently in phenology inference]\n",
    "    - paddock boundaries (*_filt.gpkg)\n",
    "2. Aggregate the time series data to paddocks; save the paddock time series  as *_paddockTS_raw.pkl \n",
    "3. Resample, interpolate and smooth paddock time series\n",
    "4. Break time series into \"paddock years\"\n",
    "5. [possible add-in for later] filter paddock years with long stretches of missing data\n",
    "6. Calculate phenology metrics for each paddock-year using phenolopy (https://github.com/lewistrotter/PhenoloPy/tree/main), which is a python implementation of TIMESAT (https://web.nateko.lu.se/timesat/timesat.asp)\n",
    "7. [possible add-in for later] predict crop type (or crop/grazing/tree/other) using a classification model that should be trained and evaluated.\n",
    "8. [possible add-in for later] predict flowering time for Canola/others (but this requires at least a rudimentary crop type classifier)\n",
    "9. Plot paddock-year time series with raw/interpolated NDVI, and mark-up the estimated Start of Season (SoS), Peak of Season (PoS) and End of Season (EoS). (save as: ______)\n",
    "10. Save the paddock phenolotrics (*_paddock_phenology_metrics.csv)\n",
    "\n",
    "The steps are currently working and it's ready to convert into a .py script. \n",
    "\n",
    "Scroll down to MAIN to see where the steps are implemented.\n",
    "\n",
    "This notebook has been tested with:\n",
    "- stub = \"TEST8\", which is 2022-2023 for a cropping region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf2e1160-2398-40c9-94c1-a5f9e60437fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of these libs are doubled up below... \n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import xarray as xr\n",
    "import rioxarray  # activate the rio accessor\n",
    "import matplotlib.pyplot as plt \n",
    "import rasterio\n",
    "import os\n",
    "import shutil\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "from affine import Affine\n",
    "from rasterstats import zonal_stats\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4e2907-c9b2-497d-bf18-94ab26e550d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for aggregating the paddock time series\n",
    "# This is a different approach to what I used previously (chatGPT), and I have not compared for efficiency. Doesn't matter for small datasets, and probably not worth worrying about. \n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from affine import Affine\n",
    "from rasterstats import zonal_stats\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "def compute_median_for_time(t, band_array, transform, geometries):\n",
    "    \"\"\"\n",
    "    Helper function that computes the zonal median for a single time step.\n",
    "    \n",
    "    Parameters:\n",
    "        t (int): The time index.\n",
    "        band_array (np.ndarray): A 3D numpy array (time, y, x) for the band.\n",
    "        transform (Affine): Affine transform for the spatial mapping.\n",
    "        geometries (list): List of shapely geometry objects.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of median values, one for each geometry.\n",
    "    \"\"\"\n",
    "    arr = band_array[t, :, :]\n",
    "    stats = zonal_stats(\n",
    "        vectors=geometries, \n",
    "        raster=arr, \n",
    "        affine=transform, \n",
    "        stats=['median'],\n",
    "        nodata=np.nan\n",
    "    )\n",
    "    return [stat['median'] for stat in stats]\n",
    "\n",
    "def summarize_medians_by_paddock(ds, pol):\n",
    "    \"\"\"\n",
    "    Computes the median of each band in the xarray dataset 'ds' for each time step,\n",
    "    over the areas defined by the geometries in the geopandas DataFrame 'pol',\n",
    "    using parallel processing to speed up the computation.\n",
    "    The resulting dataset uses the 'paddock' column (converted to string) of 'pol' as a coordinate.\n",
    "    \n",
    "    Parameters:\n",
    "        ds (xarray.Dataset): Input dataset with dimensions (time, y, x) and band variables.\n",
    "        pol (geopandas.GeoDataFrame): DataFrame with a 'geometry' column and a 'paddock' column.\n",
    "        \n",
    "    Returns:\n",
    "        xarray.Dataset: A new dataset with coordinates 'paddock' and 'time'. Each variable is\n",
    "                        named <band>_median and holds the median values computed for each paddock\n",
    "                        and time step.\n",
    "    \"\"\"\n",
    "    # Create an affine transform from the xarray coordinates.\n",
    "    x = ds.coords['x'].values\n",
    "    y = ds.coords['y'].values\n",
    "    dx = x[1] - x[0]\n",
    "    dy = y[1] - y[0]\n",
    "    transform = Affine.translation(x[0] - dx/2, y[0] - dy/2) * Affine.scale(dx, dy)\n",
    "    \n",
    "    # Convert paddock identifiers to strings.\n",
    "    paddock_labels = pol.paddock.astype(str)\n",
    "    \n",
    "    # Prepare an empty dataset using the paddock names and time coordinates.\n",
    "    ds_paddocks = xr.Dataset(coords={\n",
    "        'paddock': paddock_labels,\n",
    "        'time': ds.coords['time']\n",
    "    })\n",
    "    \n",
    "    # Prepare the list of geometries (ensuring they are picklable for parallel processing).\n",
    "    geometries = list(pol['geometry'])\n",
    "    \n",
    "    # Loop over each band variable in the dataset.\n",
    "    for band in ds.data_vars:\n",
    "        medians = np.empty((len(pol), ds.sizes['time']))\n",
    "        band_array = ds[band].values  # shape: (time, y, x)\n",
    "        \n",
    "        # Create a partial function that fixes band_array, transform, and geometries.\n",
    "        func = partial(compute_median_for_time, band_array=band_array, \n",
    "                       transform=transform, geometries=geometries)\n",
    "        \n",
    "        # Use parallel processing to compute the zonal median for each time step.\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            results = list(executor.map(func, range(ds.sizes['time'])))\n",
    "        \n",
    "        # Populate the medians array.\n",
    "        for t, med in enumerate(results):\n",
    "            medians[:, t] = med\n",
    "        \n",
    "        # Add the median values as a new variable in the output dataset.\n",
    "        ds_paddocks[band] = (('paddock', 'time'), medians)\n",
    "    \n",
    "    return ds_paddocks\n",
    "\n",
    "# Example usage:\n",
    "# ds_paddocks = summarize_medians_by_paddock(ds, pol)\n",
    "# print(ds_paddocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b9f6e58-1015-4a15-9681-fbaf0b4075e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_interpolate_smooth_paddocks(ds,\n",
    "                                         days=10,\n",
    "                                         window_length=7,\n",
    "                                         polyorder=2):\n",
    "    \"\"\"\n",
    "    Resample, conservatively interpolate, and smooth all time-dependent\n",
    "    variables in a paddock-time xarray Dataset.\n",
    "\n",
    "    The logic is identical to my function from ACT trees project:\n",
    "      1. Separate non-time-dependent variables.\n",
    "      2. Resample time-dependent data every `days` days (median).\n",
    "      3. Interpolate missing values with PCHIP (conservative).\n",
    "      4. Smooth with Savitzky–Golay.\n",
    "      5. Re-attach static variables and return the new dataset.\n",
    "\n",
    "        Parameters\n",
    "    ----------\n",
    "    ds : xarray.Dataset\n",
    "        The input dataset.\n",
    "    days : int, optional\n",
    "        The resampling frequency in days (default is 10).\n",
    "    window_length : int, optional\n",
    "        The window length for the Savitzky–Golay filter (default is 7). This value must be odd. This is how many resampled obs the polynomial is fit to. \n",
    "    polyorder : int, optional\n",
    "        The polynomial order for the Savitzky–Golay filter (default is 2). should be smaller than window_length. Higher orders let the filter follow curvature more closely but can re-introduce noise if the window is too short.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import xarray as xr\n",
    "    from scipy.interpolate import PchipInterpolator\n",
    "    from scipy.signal import savgol_filter\n",
    "\n",
    "    # ─────────────────────────── 1. split vars ────────────────────────────\n",
    "    time_dependent_vars = [v for v in ds.data_vars if \"time\" in ds[v].dims]\n",
    "    non_time_dependent_vars = [v for v in ds.data_vars\n",
    "                               if v not in time_dependent_vars]\n",
    "\n",
    "    ds_non_time = ds[non_time_dependent_vars]  # may be empty\n",
    "    ds_time_dep = ds[time_dependent_vars]\n",
    "\n",
    "    # ─────────────────── 2. resample on a fixed grid ──────────────────────\n",
    "    ds_resampled = ds_time_dep.resample(time=f\"{days}D\").median()\n",
    "    ds_resampled = ds_resampled.transpose(\"paddock\", \"time\")\n",
    "\n",
    "    # ─────────────────── 3. interpolate with PCHIP ────────────────────────\n",
    "    interp_dict = {}\n",
    "    x = np.arange(ds_resampled.time.size)\n",
    "\n",
    "    for var in time_dependent_vars:                           # loop over bands\n",
    "        data = ds_resampled[var].values                       # (paddock, time)\n",
    "        data_interp = np.empty_like(data, dtype=np.float64)\n",
    "\n",
    "        for i in range(data.shape[0]):                        # loop paddocks\n",
    "            y = data[i]\n",
    "            valid = np.isfinite(y)                            # NaN or ±Inf safe\n",
    "            if valid.sum() >= 2:\n",
    "                try:\n",
    "                    f = PchipInterpolator(x[valid], y[valid],\n",
    "                                          extrapolate=True)\n",
    "                    data_interp[i] = f(x)\n",
    "                except ValueError:        # PCHIP can still fail (e.g. all equal)\n",
    "                    data_interp[i] = np.nanmean(y) if valid.any() else np.nan\n",
    "            else:\n",
    "                data_interp[i] = np.nanmean(y) if valid.any() else np.nan\n",
    "\n",
    "        # ─────────────── 4. Savitzky–Golay smoothing ────────────────\n",
    "        wl = window_length + (window_length + 1) % 2           # make odd\n",
    "        wl = min(wl, data_interp.shape[1] | 1)                 # ≤ n_time & odd\n",
    "        data_smoothed = savgol_filter(data_interp,\n",
    "                                      window_length=wl,\n",
    "                                      polyorder=polyorder,\n",
    "                                      axis=-1)\n",
    "        interp_dict[var] = ((\"paddock\", \"time\"), data_smoothed)\n",
    "\n",
    "    # ─────────────────── 5. rebuild dataset ───────────────────────────────\n",
    "    ds_new = ds_resampled.copy()\n",
    "    for var, da in interp_dict.items():\n",
    "        ds_new[var] = da\n",
    "    for var in non_time_dependent_vars:\n",
    "        ds_new[var] = ds_non_time[var]\n",
    "\n",
    "    # keep original coords that might have been dropped (e.g. spatial_ref)\n",
    "    for c in ds.coords:\n",
    "        if c not in ds_new.coords:\n",
    "            ds_new = ds_new.assign_coords({c: ds[c]})\n",
    "\n",
    "    return ds_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a64ad1-d51a-4ee2-bb98-b356c87f2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_paddockTS_by_year(ds):\n",
    "    \"\"\"\n",
    "    Split paddock time series data by year, add day of year (doy) coordinate, and check for duplicate dates.\n",
    "    \n",
    "    Args:\n",
    "        ds (xarray.Dataset): The input dataset containing time series data for each paddock.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary where each key is a year (int), and the value is an xarray.Dataset for that year,\n",
    "              with an added 'doy' coordinate.\n",
    "    \"\"\"\n",
    "    # Identify unique years in the 'time' coordinate (sorted)\n",
    "    years = np.unique(ds.time.dt.year.values)\n",
    "    \n",
    "    # Dictionary to store datasets for each year\n",
    "    datasets_by_year = {}\n",
    "    \n",
    "    for year in years:\n",
    "        # Select data for the given year\n",
    "        ds_year = ds.sel(time=ds.time.dt.year == year)\n",
    "\n",
    "        # Useful bit of code if there are S1 data present, which can cause multi-obs on single day. \n",
    "        # # Check for multiple observations on the same date\n",
    "        # dates = ds_year.time.dt.date.values\n",
    "        # unique_dates, counts = np.unique(dates, return_counts=True)\n",
    "        # duplicate_dates = unique_dates[counts > 1]\n",
    "        \n",
    "        # if duplicate_dates.size > 0:\n",
    "        #     print(f\"Multiple observations found on date(s) in {year}: {duplicate_dates}\")\n",
    "        # else:\n",
    "        #     print(f\"All observations in {year} occur on unique dates.\")\n",
    "        \n",
    "        # Add an attribute for the year\n",
    "        ds_year.attrs['year'] = int(year)\n",
    "        \n",
    "        # Calculate day of year (doy) from 'time' and add it as a new coordinate\n",
    "        doy = ds_year.time.dt.dayofyear.data  # use .data to extract the underlying array\n",
    "        ds_year = ds_year.assign_coords(doy=('time', doy))\n",
    "        \n",
    "        # Store the dataset in the dictionary\n",
    "        datasets_by_year[int(year)] = ds_year\n",
    "\n",
    "    return datasets_by_year\n",
    "\n",
    "# Example usage:\n",
    "# ds_paddock_years = split_paddockTS_by_year(ds_paddocks_ind)\n",
    "# print(ds_paddock_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "549a6870-94b6-40b1-a1b1-1b43ec79a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## The functions in this block are no longer used, but worth keeping for now... \n",
    "# ## These attempt to recreate TIMESAT/phenolopy functions but using directly our style of paddock time series as input. \n",
    "# ## Eventually I replaced this with an approach that coerces our paddock time series data into a format directly compatible with the phenolopy functions (I prefer this updated appraoch)\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import xarray as xr\n",
    "# import math\n",
    "# from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "# def make_phenology_template(\n",
    "#     ds_by_year: dict[int, xr.Dataset]\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Build a template DataFrame for storing phenology metrics for each\n",
    "#     paddock and year.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     ds_by_year : dict[int, xr.Dataset]\n",
    "#         Mapping from year to an xarray.Dataset, each containing a\n",
    "#         'paddock' coordinate listing paddock IDs.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     pd.DataFrame\n",
    "#         A DataFrame with one row per paddock-year, and columns:\n",
    "#         - paddock: paddock ID\n",
    "#         - year: calendar year\n",
    "#         - n_peaks: number of phenological peaks (init NaN)\n",
    "#         - SoS: start of season (day-of-year, init NaN)\n",
    "#         - PoS_val: peak-of-season value (init NaN)\n",
    "#         - PoS: peak-of-season time (day-of-year, init NaN)\n",
    "#         - MOS: middle-of-season value (init NaN)\n",
    "#         - EoS: end of season (day-of-year, init NaN)\n",
    "#         - pred_crop: predicted crop type (init None)\n",
    "#     \"\"\"\n",
    "#     records: list[dict] = []\n",
    "#     for year, ds in ds_by_year.items():\n",
    "#         paddocks = ds.coords[\"paddock\"].values\n",
    "#         for pad in paddocks:\n",
    "#             records.append({\n",
    "#                 \"paddock\": pad,\n",
    "#                 \"year\": year,\n",
    "#                 # placeholder metrics:\n",
    "#                 \"n_peaks\": np.nan,\n",
    "#                 \"SoS\": np.nan,\n",
    "#                 \"PoS_val\": np.nan,\n",
    "#                 \"PoS\": np.nan,\n",
    "#                 \"MOS\": np.nan,\n",
    "#                 \"EoS\": np.nan,\n",
    "#                 \"pred_crop\": None,\n",
    "#             })\n",
    "#     cols = [\n",
    "#         \"paddock\", \"year\", \"n_peaks\", \"SoS\",\n",
    "#         \"PoS_val\", \"PoS\", \"MOS\", \"EoS\", \"pred_crop\"\n",
    "#     ]\n",
    "#     return pd.DataFrame.from_records(records, columns=cols)\n",
    "\n",
    "\n",
    "# def add_num_peaks(\n",
    "#     df: pd.DataFrame,\n",
    "#     ds_by_year: dict[int, xr.Dataset],\n",
    "#     var: str = \"NDVI\"\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Populate 'n_peaks' by counting peaks via scipy.find_peaks on each series.\n",
    "#     \"\"\"\n",
    "#     for idx, row in df.iterrows():\n",
    "#         ds = ds_by_year.get(row['year'])\n",
    "#         if ds is None or var not in ds:\n",
    "#             df.at[idx, 'n_peaks'] = 0\n",
    "#             continue\n",
    "#         series = ds[var].sel(paddock=row['paddock']).values\n",
    "#         valid = np.isfinite(series)\n",
    "#         if valid.sum() < 3:\n",
    "#             df.at[idx, 'n_peaks'] = 0\n",
    "#             continue\n",
    "#         height = np.nanquantile(series[valid], 0.75)\n",
    "#         dist = math.ceil(len(series)/4)\n",
    "#         peaks, _ = find_peaks(series, height=height, distance=dist)\n",
    "#         df.at[idx, 'n_peaks'] = len(peaks)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def add_peak_of_season(\n",
    "#     df: pd.DataFrame,\n",
    "#     ds_by_year: dict[int, xr.Dataset],\n",
    "#     var: str = \"NDVI\"\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Populate PoS_val and PoS with the max value and its day-of-year.\"\"\"\n",
    "#     for idx, row in df.iterrows():\n",
    "#         ds = ds_by_year.get(row['year'])\n",
    "#         if ds is None or var not in ds:\n",
    "#             continue\n",
    "#         da = ds[var].sel(paddock=row['paddock'])\n",
    "#         # peak value and time\n",
    "#         val = da.max('time', skipna=True).item()\n",
    "#         t_idx = da.argmax('time', skipna=True).item()\n",
    "#         time = int(da['time.dayofyear'].isel(time=t_idx))\n",
    "#         df.at[idx, 'PoS_val'] = val\n",
    "#         df.at[idx, 'PoS'] = time\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def add_middle_of_season(\n",
    "#     df: pd.DataFrame,\n",
    "#     ds_by_year: dict[int, xr.Dataset],\n",
    "#     var: str = \"NDVI\"\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Populate MOS with the mean of the upper 80% slopes around the peak.\n",
    "#     \"\"\"\n",
    "#     for idx, row in df.iterrows():\n",
    "#         ds = ds_by_year.get(row['year'])\n",
    "#         if ds is None or var not in ds:\n",
    "#             continue\n",
    "#         da = ds[var].sel(paddock=row['paddock'])\n",
    "#         # retrieve peak day-of-year from df\n",
    "#         peak_day = row['PoS']\n",
    "#         # split slopes\n",
    "#         left = da.where(da['time.dayofyear'] <= peak_day)\n",
    "#         right = da.where(da['time.dayofyear'] >= peak_day)\n",
    "#         # threshold at 80% max\n",
    "#         l80 = left.where(left >= left.max('time')*0.8)\n",
    "#         r80 = right.where(right >= right.max('time')*0.8)\n",
    "#         # average\n",
    "#         mos_val = float((l80.mean('time') + r80.mean('time')) / 2)\n",
    "#         df.at[idx, 'MOS'] = mos_val\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def add_start_of_season(\n",
    "#     df: pd.DataFrame,\n",
    "#     ds_by_year: dict[int, xr.Dataset],\n",
    "#     var: str = \"NDVI\"\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Detect SoS (first positive slope) and fill SoS.\"\"\"\n",
    "#     for idx, row in df.iterrows():\n",
    "#         ds = ds_by_year.get(row['year'])\n",
    "#         if ds is None or var not in ds:\n",
    "#             continue\n",
    "#         da = ds[var].sel(paddock=row['paddock'])\n",
    "#         peak = row['PoS']\n",
    "#         slope = da.where(da['time.dayofyear'] <= peak)\n",
    "#         diffs = slope.diff('time')\n",
    "#         pos = slope.where(diffs > 0)\n",
    "#         # choose first non-NaN\n",
    "#         times = pos['time.dayofyear'].values\n",
    "#         vals = pos.values\n",
    "#         # find first valid\n",
    "#         mask = np.isfinite(vals)\n",
    "#         if mask.any():\n",
    "#             first = np.argmax(mask)\n",
    "#             df.at[idx, 'SoS'] = int(times[first])\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def add_end_of_season(\n",
    "#     df: pd.DataFrame,\n",
    "#     ds_by_year: dict[int, xr.Dataset],\n",
    "#     var: str = \"NDVI\"\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Detect EoS as first negative slope after peak and fill EoS.\"\"\"\n",
    "#     for idx, row in df.iterrows():\n",
    "#         ds = ds_by_year.get(row['year'])\n",
    "#         if ds is None or var not in ds:\n",
    "#             continue\n",
    "#         da = ds[var].sel(paddock=row['paddock'])\n",
    "#         peak = row['PoS']\n",
    "#         slope = da.where(da['time.dayofyear'] >= peak)\n",
    "#         diffs = slope.diff('time')\n",
    "#         neg = slope.where(diffs < 0)\n",
    "#         times = neg['time.dayofyear'].values\n",
    "#         vals = neg.values\n",
    "#         mask = np.isfinite(vals)\n",
    "#         if mask.any():\n",
    "#             first = np.argmax(mask)\n",
    "#             df.at[idx, 'EoS'] = int(times[first])\n",
    "#     return df\n",
    "\n",
    "# # Example pipeline:\n",
    "# # df = make_phenology_template(ds_by_year)\n",
    "# # df = add_num_peaks(df, ds_by_year, \"nbart_red\")\n",
    "# # df = add_peak_of_season(df, ds_by_year, \"nbart_red\")\n",
    "# # df = add_middle_of_season(df, ds_by_year, \"nbart_red\")\n",
    "# # df = add_start_of_season(df, ds_by_year, \"nbart_red\")\n",
    "# # df = add_end_of_season(df, ds_by_year, \"nbart_red\")\n",
    "# # print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5abfaf5b-42a0-49d3-bb97-b51c1578dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_heatmap(ds, variable_name):\n",
    "    \"\"\"\n",
    "    Plots a heatmap for the specified variable in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        ds (xarray.Dataset): Dataset with 'paddock' and 'time' as coordinates.\n",
    "        variable_name (str): Name of the variable to plot (e.g., 'nbart_blue_median').\n",
    "    \"\"\"\n",
    "    # Convert the DataArray into a pandas DataFrame\n",
    "    # The index will be paddocks and columns will be time.\n",
    "    df = ds[variable_name].to_pandas()\n",
    "    \n",
    "    # Format the dates as YYYY-MM-DD (assuming the columns are a DatetimeIndex)\n",
    "    if isinstance(df.columns, pd.DatetimeIndex):\n",
    "        df.columns = df.columns.strftime('%Y-%m-%d')\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot the heatmap without annotations and with a shrunken colorbar.\n",
    "    ax = sns.heatmap(df, cmap=\"viridis\", annot=False, cbar_kws={\"shrink\": 0.5})\n",
    "    \n",
    "    # Set the colorbar title to the variable name.\n",
    "    colorbar = ax.collections[0].colorbar\n",
    "    colorbar.set_label(variable_name)\n",
    "    \n",
    "    # Set axis labels\n",
    "    ax.set_xlabel(\"Observation Date\")\n",
    "    ax.set_ylabel(\"Paddock\")\n",
    "    \n",
    "    # Rotate paddock (row) labels to be horizontal.\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    # No plot title is set.\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_heatmap(ds_paddocks, 'nbart_blue_median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a6ac1ba-1c2f-4cc1-8e15-40c2cef17849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will run phenolopy.calc_num_seasons and .calc_phenometrics given our resampled, interpolated \"paddock-years\" time series. \n",
    "# Must specify which variable to use, probably choose NDVI\n",
    "# Important parameters are specified in phenolopy.calc_phenometrics() with big impacts on the results\n",
    "\n",
    "# Need to replace this way of importing phenolopy with something stable across users. E.g. clone phenology into paddockts?\n",
    "import sys\n",
    "# sys.path.append('/g/data/xe2/John/Software/PhenoloPy/scripts')\n",
    "sys.path.append('//home/106/jb5097/Projects/PaddockTS/PhenoloPy/scripts')\n",
    "import phenolopy\n",
    "\n",
    "import importlib\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Reload xarray to ensure a clean state\n",
    "xr = importlib.reload(xr)\n",
    "\n",
    "# Backup the original merge\n",
    "_real_merge = xr.merge\n",
    "\n",
    "@contextmanager\n",
    "def override_xr_merge():\n",
    "    \"\"\"\n",
    "    Context manager to temporarily override xarray.merge to use compat='override'.\n",
    "    \"\"\"\n",
    "    # Define the override function\n",
    "    def _override_merge(objs, *args, **kwargs):\n",
    "        kwargs.pop(\"compat\", None)\n",
    "        return _real_merge(objs, compat=\"override\", **kwargs)\n",
    "    \n",
    "    # Patch phenolopy's xarray reference\n",
    "    phenolopy.xr.merge = _override_merge\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        # Restore the original merge\n",
    "        phenolopy.xr.merge = _real_merge\n",
    "\n",
    "def process_phenology(ds_yearly: dict, which_var: str) -> dict:\n",
    "    \"\"\"\n",
    "    For each year in ds_yearly, computes the number of seasons and phenology metrics,\n",
    "    returning a dict of pandas DataFrames (one per year).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_yearly : dict\n",
    "        Mapping from year (int) to xarray.Dataset containing paddock time series.\n",
    "    which_var : str\n",
    "        Name of the data variable to process (e.g. 'NDVI').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict of pd.DataFrame\n",
    "        One DataFrame per year, indexed by paddock, with phenology metrics and num_seasons.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for year, ds in ds_yearly.items():\n",
    "        # 1) Extract veg_index and drop doy\n",
    "        ds_veg = (\n",
    "            ds[[which_var]]\n",
    "            .rename({which_var: \"veg_index\"})\n",
    "            .drop_vars(\"doy\")\n",
    "        )\n",
    "        \n",
    "        # 2) Compute number of seasons per paddock\n",
    "        da_num_seasons = phenolopy.calc_num_seasons(ds=ds_veg)\n",
    "        \n",
    "        # 3) Compute phenometrics with monkey-patched merge\n",
    "        with override_xr_merge():\n",
    "            ds_phenos = phenolopy.calc_phenometrics(\n",
    "                da=ds_veg[\"veg_index\"],\n",
    "                peak_metric=\"pos\",\n",
    "                base_metric=\"bse\",\n",
    "                method=\"seasonal_amplitude\",\n",
    "                factor=0.05,\n",
    "                thresh_sides=\"two_sided\",\n",
    "                abs_value=0,\n",
    "            )\n",
    "        \n",
    "        # 4) Convert to DataFrame\n",
    "        phenos_df = (\n",
    "            ds_phenos\n",
    "            .drop_vars([\"spatial_ref\", \"time\"])\n",
    "            .to_dataframe()\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "        # 5) Insert num_peaks column\n",
    "        phenos_df[\"num_peaks\"] = da_num_seasons.values\n",
    "        \n",
    "        # Store\n",
    "        results[year] = phenos_df\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# phenology_results = process_phenology(ds_paddocks_resample_years, which_var=\"NDVI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb54ec3-7651-4ab3-90c1-f28188a2095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "def plot_paddock_year_phenology_with_raw(phenology_results: dict,\n",
    "                                         ds_paddocks_resample_years: dict,\n",
    "                                         ds_paddocks: xr.Dataset,\n",
    "                                         variable: str):\n",
    "    \"\"\"\n",
    "    Overlay raw and interpolated data as specified:\n",
    "    - Interpolated (smoothed) series: blue circles with solid white centers.\n",
    "    - Raw series: blue circles with solid blue centers, plotted on top.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    phenology_results : dict[int, pd.DataFrame]\n",
    "        Year -> DataFrame with 'paddock', 'sos_times', 'pos_times', 'eos_times', 'num_seasons'.\n",
    "    ds_paddocks_resample_years : dict[int, xr.Dataset]\n",
    "        Year -> Dataset with interpolated time series and 'doy'.\n",
    "    ds_paddocks : xr.Dataset\n",
    "        Raw Dataset with dims ('time', 'paddock') and datetime64 time.\n",
    "    variable : str\n",
    "        Variable name to plot (e.g., 'NDVI').\n",
    "    \"\"\"\n",
    "    years = sorted(ds_paddocks_resample_years.keys())\n",
    "    paddocks = list(ds_paddocks_resample_years[years[0]].paddock.values)\n",
    "    n_rows, n_cols = len(paddocks), len(years)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols,\n",
    "                             figsize=(4 * n_cols, 1.5 * n_rows),\n",
    "                             squeeze=False)\n",
    "\n",
    "    for i, paddock in enumerate(paddocks):\n",
    "        for j, year in enumerate(years):\n",
    "            ax = axes[i, j]\n",
    "            df_year = phenology_results[year]\n",
    "            ds_year = ds_paddocks_resample_years[year]\n",
    "\n",
    "            # Plot interpolated (smoothed) series first: blue edge, white fill\n",
    "            da_res = ds_year[variable].sel(paddock=str(paddock))\n",
    "            res_doy = ds_year['doy'].values\n",
    "            ax.scatter(res_doy, da_res.values,\n",
    "                       facecolors='white', edgecolors='blue',\n",
    "                       s=20, label='interpolated')\n",
    "\n",
    "            # Raw subset for this year and paddock\n",
    "            ds_raw_year = ds_paddocks.sel(\n",
    "                time=slice(f\"{year}-01-01\", f\"{year}-12-31\")\n",
    "            )\n",
    "            da_raw = ds_raw_year[variable].sel(paddock=str(paddock))\n",
    "            raw_doy = da_raw['time'].dt.dayofyear.values\n",
    "\n",
    "            # Plot raw points on top: solid blue circles\n",
    "            ax.scatter(raw_doy, da_raw.values,\n",
    "                       color='blue', s=20, label='raw')\n",
    "\n",
    "            # Titles and labels\n",
    "            if i == 0:\n",
    "                ax.set_title(f\"{year}\", pad=8)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(variable)\n",
    "            ax.set_xlabel(\"DOY\")\n",
    "\n",
    "            # Annotate phenology lines\n",
    "            row = df_year[df_year['paddock'].astype(str) == str(paddock)]\n",
    "            if not row.empty:\n",
    "                r = row.iloc[0]\n",
    "                ax.axvline(r['sos_times'], color='green', linestyle='--', label='SoS')\n",
    "                ax.axvline(r['pos_times'], color='blue', linestyle='-.', label='PoS')\n",
    "                ax.axvline(r['eos_times'], color='red', linestyle=':', label='EoS')\n",
    "\n",
    "                bbox = dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round')\n",
    "                ax.text(0.05, 0.9, f\"Paddock {paddock}\", transform=ax.transAxes,\n",
    "                        va='top', bbox=bbox)\n",
    "                ax.text(0.05, 0.7, f\"n_peaks: {int(r['num_peaks'])}\", transform=ax.transAxes,\n",
    "                        va='top', bbox=bbox)\n",
    "\n",
    "    # Add a single legend\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=4)\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    #plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d8b88-33cf-4e58-9d24-e6d02802efdb",
   "metadata": {},
   "source": [
    "## phenolopy parameter selection:\n",
    "from https://github.com/lewistrotter/PhenoloPy/blob/main/scripts/phenolopy.py\n",
    "\n",
    "    peak_metric: str\n",
    "        Sets the highest value for each pixel timeseries to use for calculations that rely on \n",
    "        the highest value. Can be either pos (peak of season), which is the single highest\n",
    "        value in the whole timeseries per pixel, or mos (middle of season), which is the mean \n",
    "        of the highest vales in top 90 percentile. Default is pos.\n",
    "    base_metric: str\n",
    "        Sets the lowest value for each pixel timeseries to use for calculations that rely on \n",
    "        the lowest value. Can be either vos (valley of season), which is the lowest possible\n",
    "        value in the whole timeseries per pixel, or bse (base), which is the mean of the min\n",
    "        value on the left and right slopes of the time series. Default is bse.\n",
    "    method: str\n",
    "        Sets the method used to determine pos (peak of season) and eos (end of season). Can be\n",
    "        first_of_slope, median_of_slope, seasonal_amplitude, absolute_value, relative_amplitude,\n",
    "        or stl_trend. See the get_pos and/or get_eos methods for more information.\n",
    "    factor: float (>=0 and <=1)\n",
    "        A float value between 0 and 1 which is used to increase or decrease the amplitude\n",
    "        threshold for the get_pos and get_eos seasonal_amplitude method. A factor closer \n",
    "        to 0 results in start of season nearer to min value, a factor closer to 1 results in \n",
    "        start of season closer to peak of season.\n",
    "    thresh_sides: str\n",
    "        A string indicating whether the sos value threshold calculation should be the min \n",
    "        value of left slope (one_sided) only, or use the bse/vos value (two_sided) calculated\n",
    "        earlier. Default is two_sided, as per TIMESAT 3.3. That said, one_sided is potentially\n",
    "        more robust.\n",
    "    abs_value: float\n",
    "        For absolute_value method only. Defines the absolute value in units of the vege index to\n",
    "        which sos is defined. The part of the vege slope that the absolute value hits will be the\n",
    "        sos value and time.   \n",
    "More details about method:\n",
    "\n",
    "    method: str\n",
    "        A string indicating which start of season detection method to use. Default is\n",
    "        same as TIMESAT: seasonal_amplitude. The available options include:\n",
    "        1. first_of_slope: lowest vege value of slope is eos (i.e. first lowest value).\n",
    "        2. median_of_slope: middle vege value of slope is eos (i.e. median value).\n",
    "        3. seasonal_amplitude: uses a percentage of the amplitude from base to find eos.\n",
    "        4. absolute_value: users defined absolute value in vege index units is used to find eos.\n",
    "        5. relative_amplitude: robust mean peak and base, and a factor of that area, used to find eos.\n",
    "        6. stl_trend: robust but slow - uses seasonal decomp LOESS method to find trend line and eos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c009e381-2867-4d4a-a5d2-9d9ec90e9eba",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb1a5ad5-627d-4e18-9a13-1b2921afc9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OLDER SETUP (I since changed the way it reads in the ds2.pkl to ds2i.pkl (i for indexes), and also the directories. \n",
    "# OLD=True\n",
    "# stub=\"TEST8\"\n",
    "# out_dir = \"/g/data/xe2/John/Data/PadSeg/\"\n",
    "# tmp = \"/g/data/xe2/John/Data/PadSeg/\"\n",
    "\n",
    "OLD = False\n",
    "stub=\"MILG_b04_2018-2024\"\n",
    "out_dir = \"/g/data/xe2/jb5097/PaddockTS_Results/\"\n",
    "tmp = \"/scratch/xe2/jb5097/tmp3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "929d45e2-c46c-4aa5-8438-2424aa0fdf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 24GB\n",
      "Dimensions:           (time: 480, y: 847, x: 773)\n",
      "Coordinates:\n",
      "  * time              (time) datetime64[ns] 4kB 2018-01-15T00:02:53.752000 .....\n",
      "  * y                 (y) float64 7kB -4.131e+06 -4.131e+06 ... -4.139e+06\n",
      "  * x                 (x) float64 6kB 1.432e+07 1.432e+07 ... 1.433e+07\n",
      "    spatial_ref       int32 4B 6933\n",
      "Data variables: (12/16)\n",
      "    nbart_blue        (time, y, x) float32 1GB 226.0 226.0 225.0 ... 435.0 451.0\n",
      "    nbart_green       (time, y, x) float32 1GB 389.0 389.0 372.0 ... 659.0 678.0\n",
      "    nbart_red         (time, y, x) float32 1GB 418.0 418.0 421.0 ... 692.0 680.0\n",
      "    nbart_red_edge_1  (time, y, x) float32 1GB 837.0 837.0 ... 1.147e+03\n",
      "    nbart_red_edge_2  (time, y, x) float32 1GB 1.549e+03 1.549e+03 ... 2.187e+03\n",
      "    nbart_red_edge_3  (time, y, x) float32 1GB 1.851e+03 1.851e+03 ... 2.642e+03\n",
      "    ...                ...\n",
      "    bg                (time, y, x) float64 3GB 0.03827 0.03827 ... 0.1045\n",
      "    pv                (time, y, x) float64 3GB 0.5861 0.5861 ... 0.7173 0.7294\n",
      "    npv               (time, y, x) float64 3GB 0.375 0.375 ... 0.1947 0.1672\n",
      "    NDVI              (time, y, x) float32 1GB 0.6582 0.6582 ... 0.596 0.6104\n",
      "    CFI               (time, y, x) float32 1GB 638.5 638.5 623.0 ... 938.7 967.5\n",
      "    NIRv              (time, y, x) float32 1GB 1.335e+03 1.335e+03 ... 1.716e+03\n",
      "Attributes:\n",
      "    crs:           epsg:6933\n",
      "    grid_mapping:  spatial_ref\n"
     ]
    }
   ],
   "source": [
    "filename = f\"{out_dir}{stub}_{'ds2.pkl' if OLD else 'ds2i.pkl'}\"\n",
    "\n",
    "with open(filename, \"rb\") as handle:\n",
    "    ds = pickle.load(handle)\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b967059-ac9d-4d1b-ab23-b1741a2badf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the polygons and plot:\n",
    "pol = gpd.read_file(out_dir+stub+'_filt.gpkg')\n",
    "pol['paddock'] = range(1,len(pol)+1)\n",
    "pol['paddock'] = pol.paddock.astype('category')\n",
    "#pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48729e9d-1991-4341-90ea-05abdea0882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_paddocks = summarize_medians_by_paddock(ds, pol)\n",
    "print(ds_paddocks)\n",
    "'''TO DO:\n",
    "Need to speed up this function. Try using methods from treets instead of this...\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a89963-a386-4484-b2bd-543acbb41dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the raw paddockTS\n",
    "outdir = out_dir\n",
    "out_name = os.path.join(outdir, stub + '_paddockTS_raw.pkl')\n",
    "with open(out_name, 'wb') as f:\n",
    "    pickle.dump(ds_paddocks, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#logging.info(f\"Data saved successfully to {out_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27b556-68b3-429f-8955-1038bf7aafe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving raw paddock time series data to:\", out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b48d73-611e-47cd-86ea-c46ef81d9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_paddocks_resample = resample_interpolate_smooth_paddocks(ds_paddocks,\n",
    "                                         days=10,\n",
    "                                         window_length=5,\n",
    "                                         polyorder=2)\n",
    "print(ds_paddocks_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9428a0-b6c9-48aa-8f89-de3848683879",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_paddocks_resample_years = split_paddockTS_by_year(ds_paddocks_resample)\n",
    "print(ds_paddocks_resample_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6af77-5784-46a9-be9b-587f3c7e15ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "which_var = 'NDVI' # specify which variable to use\n",
    "\n",
    "phenology_results = process_phenology(ds_paddocks_resample_years, which_var=which_var)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d1f3f61-d282-4bdd-8027-b974f13f776c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859cb350-2e05-4a46-9938-a8fc2364ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert paddock year phenology metrics to a single table and export it:\n",
    "\n",
    "def save_paddock_year_phenology(phenology_results: dict,\n",
    "                                out_dir: str,\n",
    "                                stub: str,\n",
    "                               which_var = which_var):\n",
    "    \"\"\"\n",
    "    Combines the year‐indexed phenology_results dict of DataFrames into a single DataFrame,\n",
    "    adds a 'year' column, and saves to csv\n",
    "    \"\"\"\n",
    "    # 1) Add year column and collect\n",
    "    frames = []\n",
    "    for year, df in phenology_results.items():\n",
    "        df2 = df.copy()\n",
    "        df2['year'] = year\n",
    "        frames.append(df2)\n",
    "\n",
    "    # 2) Concatenate\n",
    "    all_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # 3) Reorder columns (optional): paddock, year, then everything else\n",
    "    cols = ['paddock', 'year'] + [c for c in all_df.columns if c not in ('paddock','year')]\n",
    "    all_df = all_df[cols]\n",
    "\n",
    "    # 4) Save to CSV\n",
    "    path = os.path.join(out_dir, f\"{stub}_paddock_year_phenology_{which_var}.csv\")\n",
    "    all_df.to_csv(path, index=False)\n",
    "    print(f\"Saved: {path}\")\n",
    "    \n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f105b-ef6c-4c4b-9e67-9f6c058ca9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = save_paddock_year_phenology(phenology_results, out_dir, stub, which_var)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d615dd-a72b-4b13-a25d-01bb33f3ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "def plot_paddock_year_phenology_with_raw(phenology_results: dict,\n",
    "                                         ds_paddocks_resample_years: dict,\n",
    "                                         ds_paddocks: xr.Dataset,\n",
    "                                         variable: str):\n",
    "    \"\"\"\n",
    "    Overlay raw and interpolated data:\n",
    "    - Interpolated: blue edge, white fill\n",
    "    - Raw: solid blue circles, plotted on top\n",
    "    Layout arranged by paddock rows and year columns.\n",
    "\n",
    "    Customizations:\n",
    "    1) X-axis label \"DOY\" only on bottom row panels.\n",
    "    2) Paddock ID printed to the right of the rightmost panel.\n",
    "    3) Y-axis limits fixed to [0,1]; tick labels only on leftmost column.\n",
    "    \"\"\"\n",
    "    years = sorted(ds_paddocks_resample_years.keys())\n",
    "    paddocks = list(ds_paddocks_resample_years[years[0]].paddock.values)\n",
    "    n_rows, n_cols = len(paddocks), len(years)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols,\n",
    "                             figsize=(4 * n_cols, 1.5 * n_rows),\n",
    "                             squeeze=False)\n",
    "\n",
    "    for i, paddock in enumerate(paddocks):\n",
    "        for j, year in enumerate(years):\n",
    "            ax = axes[i, j]\n",
    "            df_year = phenology_results[year]\n",
    "            ds_year = ds_paddocks_resample_years[year]\n",
    "\n",
    "            # Interpolated series\n",
    "            da_res = ds_year[variable].sel(paddock=str(paddock))\n",
    "            res_doy = ds_year['doy'].values\n",
    "            ax.scatter(res_doy, da_res.values,\n",
    "                       facecolors='white', edgecolors='blue',\n",
    "                       s=20, label='interpolated')\n",
    "\n",
    "            # Raw series\n",
    "            ds_raw_year = ds_paddocks.sel(\n",
    "                time=slice(f\"{year}-01-01\", f\"{year}-12-31\")\n",
    "            )\n",
    "            da_raw = ds_raw_year[variable].sel(paddock=str(paddock))\n",
    "            raw_doy = da_raw['time'].dt.dayofyear.values\n",
    "            ax.scatter(raw_doy, da_raw.values,\n",
    "                       color='blue', s=20, label='raw')\n",
    "\n",
    "            # Phenology lines\n",
    "            row = df_year[df_year['paddock'].astype(str) == str(paddock)]\n",
    "            if not row.empty:\n",
    "                r = row.iloc[0]\n",
    "                ax.axvline(r['sos_times'], color='green', linestyle='--', label='SoS')\n",
    "                ax.axvline(r['pos_times'], color='blue', linestyle='-.', label='PoS')\n",
    "                ax.axvline(r['eos_times'], color='red', linestyle=':', label='EoS')\n",
    "\n",
    "            # Y-axis limits and tick labels\n",
    "            ax.set_ylim(0, 1)\n",
    "            if j == 0:\n",
    "                ax.tick_params(labelleft=True)\n",
    "                ax.set_ylabel(variable)\n",
    "            else:\n",
    "                ax.tick_params(labelleft=False)\n",
    "\n",
    "            # X-axis label only bottom row\n",
    "            if i == n_rows - 1:\n",
    "                ax.set_xlabel(\"DOY\")\n",
    "            else:\n",
    "                ax.tick_params(labelbottom=False)\n",
    "\n",
    "            # Title on top row\n",
    "            if i == 0:\n",
    "                ax.set_title(f\"{year}\", pad=8)\n",
    "\n",
    "            # Paddock ID on rightmost column\n",
    "            if j == n_cols - 1:\n",
    "                ax.text(1.02, 0.5, f\"Paddock {paddock}\",\n",
    "                        transform=ax.transAxes, va='center')\n",
    "\n",
    "    # Single legend\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=4)\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac218f6-0442-49ae-ae20-d3bf2123abd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_paddock_year_phenology_with_raw(\n",
    "    phenology_results,\n",
    "    ds_paddocks_resample_years,\n",
    "    ds_paddocks,\n",
    "    variable=\"NDVI\"\n",
    ")\n",
    "\n",
    "fig.savefig(out_dir+stub+\"_paddock_year_phenology_\"+which_var+\".svg\", format=\"svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d514282-4d1d-497f-a697-4a3bc887e72c",
   "metadata": {},
   "source": [
    "## More analysis\n",
    "\n",
    "Below is stuff that is interesting to keep in the notebook but not neccesary to move into the .py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b94a6ae-f5df-41e6-b06a-057df37aac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(ds_paddocks, 'NDVI')\n",
    "plot_heatmap(ds_paddocks_resample, 'NDVI')\n",
    "# easy to predict which paddocks had canola\n",
    "#plot_heatmap(ds_paddocks_resample_years[2023], 'CFI')\n",
    "plot_heatmap(ds_paddocks_resample, 'CFI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a6fc6-5913-4c36-92ed-b06a9164fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Purpose of this function is to explore missing data in paddock time series. \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_gap_lengths_by_year(ds: xr.Dataset,\n",
    "                             var: str = \"nbart_red\",\n",
    "                             *,\n",
    "                             unit_dim: str = \"paddock\",\n",
    "                             time_dim: str = \"time\",\n",
    "                             min_gap_days: int = 1,\n",
    "                             bins: int | str = \"auto\",\n",
    "                             figsize: tuple[int, int] = (6, 4),\n",
    "                             show: bool = True) -> dict[int, list[int]]:\n",
    "    \"\"\"\n",
    "    Histogram of calendar-day gaps between successive valid observations.\n",
    "\n",
    "    • Left y-axis  : absolute count of gaps.\n",
    "    • Right y-axis : count ÷ number-of-paddocks (tick labels rounded).\n",
    "\n",
    "    A red dotted vertical line marks `min_gap_days`; x-axis spans [min, max].\n",
    "    \"\"\"\n",
    "    if var not in ds.data_vars:\n",
    "        raise KeyError(f\"{var!r} not found in dataset.\")\n",
    "\n",
    "    da = ds[var]\n",
    "    n_units = ds.dims[unit_dim]    # total paddocks\n",
    "\n",
    "    def _gaps_in_days(times: np.ndarray) -> np.ndarray:\n",
    "        if times.size < 2:\n",
    "            return np.empty(0, dtype=int)\n",
    "        deltas = np.diff(np.sort(times.astype(\"datetime64[D]\")))\n",
    "        return deltas.astype(\"timedelta64[D]\").astype(int)\n",
    "\n",
    "    gap_dict: dict[int, list[int]] = {}\n",
    "\n",
    "    for yr, da_year in da.groupby(f\"{time_dim}.year\"):\n",
    "        gaps_all: list[int] = []\n",
    "        for unit in da_year[unit_dim]:\n",
    "            times = da_year.sel({unit_dim: unit}).dropna(time_dim)[time_dim].values\n",
    "            gaps_all.extend(_gaps_in_days(times))\n",
    "\n",
    "        gap_dict[yr] = [g for g in gaps_all if g >= min_gap_days]\n",
    "\n",
    "        if show:\n",
    "            fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "            # histogram\n",
    "            counts, edges, _ = ax.hist(gaps_all, bins=bins,\n",
    "                                       edgecolor=\"black\", alpha=0.7)\n",
    "\n",
    "            # secondary y-axis: same scale, labels = counts/n_units (rounded)\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.set_ylim(ax.get_ylim())\n",
    "            primary_ticks = ax.get_yticks()\n",
    "            ax2.set_yticks(primary_ticks)\n",
    "            ax2.set_yticklabels([str(int(round(y / n_units))) for y in primary_ticks])\n",
    "            ax2.set_ylabel(\"gaps per paddock\")\n",
    "\n",
    "            # red dotted threshold\n",
    "            ax.axvline(min_gap_days, color=\"red\", linestyle=\"dotted\",\n",
    "                       linewidth=1.5, label=f\"threshold = {min_gap_days} d\")\n",
    "\n",
    "            # axis limits\n",
    "            if gaps_all:\n",
    "                ax.set_xlim(min(gaps_all), max(gaps_all))\n",
    "            else:\n",
    "                ax.set_xlim(0, 1)\n",
    "\n",
    "            ax.set_xlabel(\"gap length (days)\")\n",
    "            ax.set_ylabel(\"number of gaps\")\n",
    "            ax.set_title(f\"{var} – gap lengths, {yr}\")\n",
    "            ax.legend(loc=\"upper right\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    return gap_dict\n",
    "\n",
    "gap_stats = plot_gap_lengths_by_year(ds_paddocks,\n",
    "                                     var=\"nbart_red\",\n",
    "                                     min_gap_days=21)\n",
    "# Next time, the threshold should indicate a red dotted line, and the domain should cover min to max. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
